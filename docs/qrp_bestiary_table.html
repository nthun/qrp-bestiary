<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<style>body{background-color:white;}</style>
<link href="lib/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="lib/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="lib/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="lib/datatables-binding-0.33/datatables.js"></script>
<script src="lib/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="lib/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="lib/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="lib/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<script src="lib/jszip-1.13.6/jszip.min.js"></script>
<script src="lib/pdfmake-1.13.6/pdfmake.js"></script>
<script src="lib/pdfmake-1.13.6/vfs_fonts.js"></script>
<link href="lib/dt-ext-buttons-1.13.6/css/buttons.dataTables.min.css" rel="stylesheet" />
<script src="lib/dt-ext-buttons-1.13.6/js/dataTables.buttons.min.js"></script>
<script src="lib/dt-ext-buttons-1.13.6/js/buttons.html5.min.js"></script>
<script src="lib/dt-ext-buttons-1.13.6/js/buttons.colVis.min.js"></script>
<script src="lib/dt-ext-buttons-1.13.6/js/buttons.print.min.js"></script>
<link href="lib/dt-ext-responsive-1.13.6/css/responsive.dataTables.min.css" rel="stylesheet" />
<script src="lib/dt-ext-responsive-1.13.6/js/dataTables.responsive.min.js"></script>
<link href="lib/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="lib/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="lib/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="lib/selectize-0.12.0/selectize.min.js"></script>
<link href="lib/dt-plugin-searchhighlight-1.13.6/source.css" rel="stylesheet" />
<script src="lib/dt-plugin-searchhighlight-1.13.6/jquery.highlight.js"></script>
<script src="lib/dt-plugin-searchhighlight-1.13.6/source.min.js"></script>
<link href="lib/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="lib/crosstalk-1.2.1/js/crosstalk.min.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/1.13.6/css/dataTables.bootstrap5.min.css"/>
</head>
<body>
<style>
    h1 {
      text-align: center;
      font-family: Roboto, Arial, sans-serif;
      color: #333;
      margin-bottom: 16px;
    }
    table.dataTable td, table.dataTable th {
      font-family: Roboto, Arial, sans-serif;
      font-size: 14px;
    }
  </style>
<h1>QRP Bestiary</h1>
<div class="datatables html-widget html-fill-item" id="htmlwidget-9494adbf095a951bb549" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-9494adbf095a951bb549">{"x":{"filter":"top","vertical":false,"filterHTML":"<tr>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n<\/tr>","extensions":["Buttons","Responsive"],"data":[["<p>Choosing biased manipulations<\/p>\n","<p>Choosing biased measurements<\/p>\n","<p>Choosing overlapping measures to find significant results<\/p>\n","<p>PARKing (preregistering after results are known)<\/p>\n","<p>Performing inappropriate power analysis<\/p>\n","<p>Grooming participants<\/p>\n","<p>Mixing pilot and main study data<\/p>\n","<p>Optional stopping<\/p>\n","<p>Selective sampling<\/p>\n","<p>Discretizing continuous variables<\/p>\n","<p>Excluding data points<\/p>\n","<p>Missing data hacking<\/p>\n","<p>Modifying measurements<\/p>\n","<p>Redefining group membership rules<\/p>\n","<p>Using ad hoc exclusion criteria for participants<\/p>\n","<p>Variable transformation fishing<\/p>\n","<p>Choosing a poor model specification<\/p>\n","<p>Choosing unjustified p-value adjustment<\/p>\n","<p>Neglecting assumptions for statistical models<\/p>\n","<p>Selecting a favorable random number generator seed<\/p>\n","<p>Selective test reporting<\/p>\n","<p>Using ad hoc covariates<\/p>\n","<p>Citing unreliable research<\/p>\n","<p>HARKing (hypothesizing after the results are known)<\/p>\n","<p>Incorrect reporting of test statistics<\/p>\n","<p>Making unsupported conclusions<\/p>\n","<p>Not disclosing deviations from preregistration<\/p>\n","<p>Omitting important details of the scientific process<\/p>\n","<p>Selective citing<\/p>\n","<p>Selective reporting of hypotheses<\/p>\n","<p>Selective reporting of indicator variables<\/p>\n","<p>Selective reporting of outcomes<\/p>\n","<p>Using irrelevant references<\/p>\n","<p>Using unjustified references<\/p>\n","<p>Visualizing data in a misleading way<\/p>\n","<p>Creating multiple publications from the same study<\/p>\n","<p>Declaring false authorship<\/p>\n","<p>Not linking the preregistration to the published study<\/p>\n","<p>Not making data accessible<\/p>\n","<p>Publishing studies selectively<\/p>\n"],["<p>—<\/p>\n","<p>—<\/p>\n","<p>Leveraging the jangle fallacy<\/p>\n","<p>—<\/p>\n","<p>—<\/p>\n","<p>—<\/p>\n","<p>Double dipping,\nRetaining pilot data<\/p>\n","<p>Peeking,\nData peeking<\/p>\n","<p>Biased sampling<\/p>\n","<p>Dichotimizing variables,\nMedian split<\/p>\n","<p>—<\/p>\n","<p>Favorable imputation<\/p>\n","<p>—<\/p>\n","<p>—<\/p>\n","<p>—<\/p>\n","<p>—<\/p>\n","<p>Overfitting or underfitting models,\nBias-variance tradeoff<\/p>\n","<p>Not adjusting or over-adjusting p-values<\/p>\n","<p>—<\/p>\n","<p>Resampling lottery<\/p>\n","<p>—<\/p>\n","<p>Selectively including control variables<\/p>\n","<p>—<\/p>\n","<p>Texas sharpshooter fallacy,\nPost hoc ergo propter hoc<\/p>\n","<p>—<\/p>\n","<p>—<\/p>\n","<p>—<\/p>\n","<p>Incomplete methods or results section<\/p>\n","<p>Cherry-picking citations<\/p>\n","<p>Cherry-picking hypotheses,\nChrysalis effect,\nFishing expedition<\/p>\n","<p>Cherry-picking indicator variables,\nCherry-picking conditions/ groups<\/p>\n","<p>Cherry-picking outcomes<\/p>\n","<p>—<\/p>\n","<p>—<\/p>\n","<p>—<\/p>\n","<p>Publication overlap, Salami slicing<\/p>\n","<p>—<\/p>\n","<p>—<\/p>\n","<p>—<\/p>\n","<p>File drawer problem<\/p>\n"],["<p>Selecting an unjustified manipulation to reach a misleading outcome.<\/p>\n","<p>Selecting an instrument that is biased or invalid, to support a desired narrative.<\/p>\n","<p>Exploiting the conceptual similarity between measures, presenting them as distinct constructs, to get significant results.<\/p>\n","<p>Preregistering a hypothesis or analysis after knowing the outcome of the analysis.<\/p>\n","<p>Selecting inappropriate parameters or methods for power analysis and/ or misinterpreting/misusing power analysis results.<\/p>\n","<p>Affecting participants to make them give responses that support the desired narrative.<\/p>\n","<p>Including data from a pilot study if the results support the hypothesis.<\/p>\n","<p>Monitoring hypothesis tests during data collection, and stopping when statistical inference is favorable, without controlling for sequential testing.<\/p>\n","<p>Collecting a sample in a way that biases the findings.<\/p>\n","<p>Taking a continuous variable and making it categorical without proper justification and transparent reporting.<\/p>\n","<p>Exclusion of data points or outliers without proper justification and transparent reporting.<\/p>\n","<p>Choosing the strategy to handle missing data based on the impact on the results.<\/p>\n","<p>Changing the properties of a measure/measurement to produce favorable results without proper justification and/or transparent reporting.<\/p>\n","<p>Post hoc (re)definition of grouping criteria without proper justification and transparent reporting.<\/p>\n","<p>Exclusion of participants without proper justification transparent reporting.<\/p>\n","<p>Selecting variable transformations that produce favorable results without proper justification and/or transparent reporting.<\/p>\n","<p>Creating too complex models on too small datasets causing the model to learn the noise and random fluctuations instead of generalizable patterns. Alternatively, creating too simple models that do not adequately fit the data.<\/p>\n","<p>Not adjusting or over-adjusting p-values when running multiple tests.<\/p>\n","<p>Using statistical models although requirements are not met.<\/p>\n","<p>Trying different random seeds until getting a favorable result, potentially in combination with small number of replications<\/p>\n","<p>Repeatedly testing a hypothesis in different ways until the desired result is found, and then selectively reporting the findings that support the desired conclusion.<\/p>\n","<p>Addition or removal of covariates to influence the estimates or significance for the effect of interest.<\/p>\n","<p>Citing an unreliable publication to support the study's narrative.<\/p>\n","<p>Presenting a hypothesis that is based on observed results (post-hoc or a posteriori) as if it was presumed before obtaining results (a priori).<\/p>\n","<p>Not using statistical test reporting conventions to obscure exact results and assume that they are above or below threshold values.<\/p>\n","<p>Interpreting research findings or their implications in a way that is not backed by evidence.<\/p>\n","<p>Deviating from the preregistration without transparency and proper justification in the publication.<\/p>\n","<p>Not reporting important details of the methodology and statistical analysis.<\/p>\n","<p>Avoiding to mention studies that do not support the hypothesis of the research or even those that do support the hypotheses to make the study appear more novel.<\/p>\n","<p>Reporting hypothesis test only if it fits the researcher's expectation.<\/p>\n","<p>Reporting only the indicator variables (or predictors, features, independent variables) that are used in analyses that produce expected results.<\/p>\n","<p>Reporting only the outcomes (or dependent variables) that are used in analyses that produce expected results.<\/p>\n","<p>Using citations that are not connected to the claims to increase the credibility of a statement.<\/p>\n","<p>Selectively citing works by specific researchers or journals to inflate citation metrics or boost a journal’s impact factor.<\/p>\n","<p>Choosing suboptimal visualizations or altering figure properties in order to exaggerate or diminish effects.<\/p>\n","<p>Breaking up of research findings from the same dataset into several publications without proper justification and the disclosing of related papers.<\/p>\n","<p>Attribution and arrangement of authorship that does not correspond to the authors’ contributions, in order to influence the publishing process, and increase the credibility of the study.<\/p>\n","<p>Creating a preregistration but not associating it with the published study.<\/p>\n","<p>The datasets and/or codebooks are not made accessible to the public and/or peer-reviewers without justifiable cause.<\/p>\n","<p>Choosing which study to publish or share based on whether the findings fit expectations.<\/p>\n"],["<ul>\n<li>Influencing participants<\/li>\n<\/ul>\n","<ul>\n<li>Influencing participants<\/li>\n<\/ul>\n","<p>None<\/p>\n","<ul>\n<li>Misusing open science practices<\/li>\n<\/ul>\n","<p>None<\/p>\n","<ul>\n<li>Influencing participants<\/li>\n<\/ul>\n","<ul>\n<li>Sample curation<\/li>\n<\/ul>\n","<ul>\n<li>Sample curation<\/li>\n<\/ul>\n","<ul>\n<li>Sample curation<\/li>\n<\/ul>\n","<ul>\n<li>P-hacking<\/li>\n<\/ul>\n","<ul>\n<li>P-hacking<\/li>\n<\/ul>\n","<ul>\n<li>P-hacking<\/li>\n<\/ul>\n","<ul>\n<li>P-hacking<\/li>\n<\/ul>\n","<ul>\n<li>P-hacking<\/li>\n<\/ul>\n","<ul>\n<li>P-hacking<\/li>\n<li>Sample curation<\/li>\n<\/ul>\n","<ul>\n<li>P-hacking<\/li>\n<\/ul>\n","<ul>\n<li>P-hacking<\/li>\n<\/ul>\n","<ul>\n<li>P-hacking<\/li>\n<\/ul>\n","<ul>\n<li>P-hacking<\/li>\n<\/ul>\n","<ul>\n<li>P-hacking<\/li>\n<\/ul>\n","<ul>\n<li>P-hacking<\/li>\n<li>Cherry-picking<\/li>\n<\/ul>\n","<ul>\n<li>P-hacking<\/li>\n<\/ul>\n","<ul>\n<li>Citation engineering<\/li>\n<\/ul>\n","<p>None<\/p>\n","<p>None<\/p>\n","<p>None<\/p>\n","<ul>\n<li>Misusing open science practices<\/li>\n<\/ul>\n","<ul>\n<li>Cherry-picking<\/li>\n<\/ul>\n","<ul>\n<li>Citation engineering<\/li>\n<li>Cherry-picking<\/li>\n<\/ul>\n","<ul>\n<li>Cherry-picking<\/li>\n<\/ul>\n","<ul>\n<li>Cherry-picking<\/li>\n<\/ul>\n","<ul>\n<li>Cherry-picking<\/li>\n<\/ul>\n","<ul>\n<li>Citation engineering<\/li>\n<\/ul>\n","<ul>\n<li>Citation engineering<\/li>\n<\/ul>\n","<p>None<\/p>\n","<ul>\n<li>Citation engineering<\/li>\n<\/ul>\n","<ul>\n<li>Citation engineering<\/li>\n<\/ul>\n","<p>None<\/p>\n","<ul>\n<li>Misusing open science practices<\/li>\n<\/ul>\n","<ul>\n<li>Cherry-picking<\/li>\n<\/ul>\n"],["<p>Planning<\/p>\n","<p>Planning<\/p>\n","<p>Planning<\/p>\n","<p>Planning<\/p>\n","<p>Planning<\/p>\n","<p>Data collection<\/p>\n","<p>Data collection<\/p>\n","<p>Data collection<\/p>\n","<p>Data collection<\/p>\n","<p>Data processing<\/p>\n","<p>Data processing<\/p>\n","<p>Data processing<\/p>\n","<p>Data processing<\/p>\n","<p>Data processing<\/p>\n","<p>Data processing<\/p>\n","<p>Data processing<\/p>\n","<p>Data analysis<\/p>\n","<p>Data analysis<\/p>\n","<p>Data analysis<\/p>\n","<p>Data analysis<\/p>\n","<p>Write-up<\/p>\n","<p>Data analysis<\/p>\n","<p>Write-up<\/p>\n","<p>Write-up<\/p>\n","<p>Write-up<\/p>\n","<p>Write-up<\/p>\n","<p>Write-up<\/p>\n","<p>Write-up<\/p>\n","<p>Write-up<\/p>\n","<p>Write-up<\/p>\n","<p>Write-up<\/p>\n","<p>Write-up<\/p>\n","<p>Write-up<\/p>\n","<p>Write-up<\/p>\n","<p>Write-up<\/p>\n","<p>Publication<\/p>\n","<p>Publication<\/p>\n","<p>Publication<\/p>\n","<p>Publication<\/p>\n","<p>Publication<\/p>\n"],["<p>(1) Selecting sub-clinical drug doses to suggest no effect or only deliberately high doses to suggest an adverse effect.\n(2) Using images or videos to elicit emotions, but the stimuli are not eliciting emotions.<\/p>\n","<p>(1) Researcher uses loaded, leading, or suggestive questions, e.g., &quot;Does the lack of respect schoolchildren have for their teachers, in your opinion, influence everyday teaching methods in schools?&quot;\n(2) Researcher assesses a psychological construct using an ad hoc questionnaire with no proven validity.\n(3) Researcher uses a scale that do not capture the intended construct properly in order to support a desired narrative<\/p>\n","<p>(1) Researcher is using similar items in two seemingly different constructs and concludes that the constructs have a high correlation.\n(2) Researcher finds a positive relationship between depression and suicidality, but the positive relation is in part due to the depression scale already containing items about suicide.\n(3) A researcher estimates a positive relation between construct A and construct B in a meta-analysis, but the meta-analysis includes studies that have used the same task to operationalize either construct.<\/p>\n","<p>(1) Researcher realizes that their paper won't be accepted without a preregistration, so they create one post-hoc and link it to their study.<\/p>\n","<p>(1) A researcher chooses a small sample size to get null results in a study about the harmful effects of smoking.\n(2) A researcher uses default parameters for the power analysis to show that power analysis had been performed, however, the analysis is uninformative.<\/p>\n","<p>(1)     Researcher tells the participants that he believes the treatment will work.\n(2)     Researcher uses a briefing document that is trying to influence participants’ attitudes about the topic that is assessed in the study.\n(3)     During data collection the same organization that runs the study also runs a marketing campaign to influence public opinion on the same topic.<\/p>\n","<p>(1) Researcher conducts a pilot study to check the protocol and analyzes pilot data. The pilot data and main study data are aggregated in the data analysis if pilot data results are in line with expectations.<\/p>\n","<p>(1) Researcher is collecting responses and tests the hypothesis after every participant - when significance is reached, the researcher stops collecting data.<\/p>\n","<p>(1) Researcher tests the likeability of chocolate on a group of children only in order to find that everyone loves it.\n(2) Using uncomparable groups: The researcher tests if men are more aggressive than women. For comparison, women from a university are compared with men from a prison.\n(3) Picking a subsample of a panel dataset to find the desired results.<\/p>\n","<p>(1)     Researcher doesn’t find an association between depression and continuous age variables, and recodes age into young and old categories. After that, age groups show a significant association with depression. An independent samples t-test is reported instead of a correlation.<\/p>\n","<p>(1) Removing individual reaction time trials based on post hoc criteria.\n(2) Trying different outlier cut-off criteria until an effect is statistically significant.<\/p>\n","<p>(1)     A researcher tries three ways of handling missing data, for example, listwise deletion, multiple imputation, and inverse probability weighting. The expected results only appear with inverse probability weighting. The researcher reports only this strategy in the paper and leaves out results with listwise deletion and multiple imputation.\n(2)     Can also be within a single method, specifically multiple imputation, since it uses one or more variables to replace missing data, and the choice of these variables is up to the researcher, but can also be statistically based.<\/p>\n","<p>(1) Researcher uses only a portion of the items from a longer scale.\n(2) Researcher combines items from different scales into a single measure.\n(3) Researcher chooses which EEG electrodes to aggregate based on the results.<\/p>\n","<p>(1) Collapsing the multicategorical variable of sexual orientation into heterosexual and non-heterosexual.\n(2) Trying different age ranges in cross-sectional age comparisons to maximize group differences.<\/p>\n","<p>(1) Researcher finds that a correlation between two variables is not significant. After removing two participants - who should be included - the association becomes significant. Then the researcher comes up with post hoc exclusion criteria for those participants.\n(2) A researcher doesn’t find an expected association between perceived stress and personality. When looking only at the top 25% of perceived stress scores, the association is there. They go on to report the top 25% scores as their population of interest and do not disclose that they looked at the rest of the sample population.<\/p>\n","<p>(1) Researcher runs a statistical test using several different transformations (e.g., changing levels of measurement, log-transformations, rescaling) of the outcome, and only reports the one that produces a significant result.<\/p>\n","<p>(1)     Overfitting: The researcher fits a regression model with 25 predictors on a sample of 100 participants.\n(2)     Underfitting: The researcher uses linear regression to investigate a non-linear association.<\/p>\n","<p>A researcher decides (1) whether or not to adjust for multiple tests (e.g., in an ANOVA) and (2) which adjustment method to use, and (3) which (i.e. how many) comparisons to include depending on results obtained.\n(4) Researcher uses Bonferroni correction when correlating several variables, to prove that an association does not exist.<\/p>\n","<p>(1) Analyzing data, using parametric tests such as t-tests but the data requires a non-parametric test.\n(2) Analyzing dependent data using a statistical model that does not account for dependency.<\/p>\n","<p>(1) A researcher keeps on bootstrapping a confidence interval (e.g., for a mediation indirect effect) with different seeds until the 95% confidence interval just excludes 0.<\/p>\n","<p>Researcher analyzes the data using (1) multiple statistical methods (multiple t-tests, ANOVAs, different random structures in LMEMs) and/ or (2) multiple data eligibility specifications. Based on the results, they choose to present only one analysis that gives a significant result.<\/p>\n","<p>(1) A researcher opportunistically decides which background variables (e.g., age, gender) to control for, without a causal theory or a preregistration.\n(2) A researcher decides whether to control for a baseline value in an experimental design depending on the results of statistical tests.\n(3) Researcher avoids the inclusion (or measurement) of theoretically justified moderators (e.g., severity of a condition, or socio-economic status) to be able to imply greater generalisability.<\/p>\n","<p>(1) Researcher cites a publication that presents low-level evidence to support a claim with no reference to the study’s limitations or no reference to other studies.\n(2) The researcher cites a retracted paper.<\/p>\n","<p>(1) Researcher claims to have predicted an unexpected result.\n(2) Researcher has no hypotheses originally and forms hypotheses after exploring the data and presenting the hypotheses as they had those from the beginning.\n(3) Researcher has a hypothesis (e.g., a mediation hypothesis) and tests it, and if the results do not confirm the hypothesis but rather indicate an alternative pattern (e.g., a moderation), the researcher claims that this is what they hypothesized all along.\n(4) Post-hoc directional hypotheses: The researcher presents a hypothesis as if it was uni-directional (i.e. group A’s mean is larger than group B’s, or a correlation will be positive), although the original hypothesis was bi-directional. This change will make the hypothesis test significant.<\/p>\n","<p>(1)     Researcher is ‘rounding off’ a p-value in a paper (e.g., reporting that a p-value of .054 is less or equal to .05).\n(2)     Researcher reports p-values only and conceals test statistics.\n(3)     Researcher reports a correlation without disclosing the degrees of freedom, number of observations, or confidence interval, so it seems like the effect is large (for example r=.70, n=15, CI=[.01; .90]).\n(4)     Researcher does a model comparison and only reports fit statistics that are in favor of the preferred model.<\/p>\n","<p>(1) Researcher concludes that a treatment is effective for groups and contexts that were not considered in the study.\n(2) Researcher concludes that a treatment worked, however, the treatment effect did not differ from the effect of the control condition (or no control condition was used).\n(3) Researcher implies causality based on a research design or that does not allow causal inference.<\/p>\n","<p>(1) Researcher preregisters that they would collect data from a non-student sample, but ends up including students, and does not disclose this deviation.\n(2) Researcher preregisters a data analysis using linear regression but used robust regression instead without reporting the discrepancy.\n(3) See also Example 1 in Selective reporting of hypotheses.<\/p>\n","<p>(1) Researcher omits sample characteristics, such as the sample was recruited on MTurk or participants received compensation for participation.\n(2) Researcher reports correlations without specifying the type, e.g., Spearman.\n(3) Researcher does not share study materials on request or does not report exact questionnaire items.\n(4) Researcher fails to mention pilot studies that were conducted to arrive at the final design.<\/p>\n","<p>(1) Researcher overly cites empirical work that supports their hypotheses and withholds citing work that did not find the effect at all or even the opposite.\n(2) Researcher omits other null findings to maximize the perceived value of a null finding.<\/p>\n","<p>(1) Researcher formulates five hypotheses of which only three are supported by the data - only these 3 get reported in the final research report (Chrysalis effect).\n(2) Fishing expedition - The researcher surveys college students about the outfit they are wearing and their scores on several tests which allows for many possible analyses (examining different colors, types of clothing, tests, score cutoffs, etc.). They end up reporting only a subset of findings to claim college students perform significantly better on tests when they are wearing green. See also Modifying measurement, Selective reporting of indicator variables, and Selective reporting of outcomes.<\/p>\n","<p>(1) Researcher reports indicators that are associated with the outcome rather than including all measured indicator variables in the results section.\n(2) Researcher drops one or more conditions or groups/merges two or more groups into one / splits a group into more groups than were initially planned depending on statistical results.<\/p>\n","<p>(1) Researcher uses several scales to measure the same construct but only reports the one that produces expected results.\n(2) Researcher tests effectiveness of a new intervention for depression by measuring its effects on anxiety, sleep quality, and stress and only reports the outcome that shows the desired effect.<\/p>\n","<p>(1) Researcher supports a statement with three citations and two of them are unrelated to the statement.<\/p>\n","<p>(1) Researcher selectively cites their own publications for boosted citation metrics.\n(2) Citation networks: Researcher cites a colleague’s unrelated work in order to get cited in a similar way.<\/p>\n","<p>(1) Researcher truncates the y-axis so it is not starting at zero and/or does not add error bars. This makes differences seem larger and more significant than they are in reality.\n(2) Researcher uses arbitrary categories to present interval data on a map.\n(3) Researcher displays a pie chart with percentage numbers falling below or exceeding 100.<\/p>\n","<p>(1) Researcher conducts a study measuring several outcomes (or predictors) and publishes results in several papers with each paper focusing on just one outcome (or predictor), while not disclosing the other papers.\n(2) A study on cross-cultural differences with 20 participating labs from 20 countries results in 10 publications where in each one two countries are compared.<\/p>\n","<p>(1) Honorary authorship: Researcher adds a co-author who did not contribute to the manuscript.\n(2) Ghost authorship: The researcher excludes a co-author who significantly contributed to the project.\n(3) Controversial researcher writes a paper and publishes it under a pseudonym, so it seems that more than one person shares the same view.<\/p>\n","<p>(1) Researcher preregisters a study and after conducting the research the preregistration is not mentioned in the manuscript because of too many diversions.<\/p>\n","<p>(1) Researcher doesn’t provide a publicly accessible repository link to the dataset.\n(2) Data repository link is accessible, but the data is not comprehensible (e.g. lacks cleaning and organization, codebook and instructions, etc), hence it is difficult or impossible to use to reproduce findings.<\/p>\n","<p>(1) Researcher runs a study and finds out the results do not support their hypothesis (e.g., no significant findings). Thus the researcher does not try to publish or share the study publicly.\n(2) Researcher runs several studies, and publishes only those that support the hypothesis in a multi-study paper.<\/p>\n"],["<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Compromised generalizability<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Reduced replicability<\/li>\n<\/ul>\n","<ul>\n<li>Inflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Reduced replicability<\/li>\n<\/ul>\n","<ul>\n<li>Inflated confidence in the research<\/li>\n<\/ul>\n","<ul>\n<li>Inflated confidence in the research<\/li>\n<li>Inflated type II error<\/li>\n<li>Reduced replicability<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Reduced replicability<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Reduced replicability<\/li>\n<\/ul>\n","<ul>\n<li>Inflated type I error<\/li>\n<li>Reduced replicability<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Compromised generalizability<\/li>\n<li>Reduced replicability if sampling bias is not disclosed<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Reduced replicability<\/li>\n<li>Compromised generalizability<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Compromised generalizability<\/li>\n<li>Reduced replicability<\/li>\n<li>Reduced reproducibility<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Reduced reproducibility<\/li>\n<li>Reduced replicability<\/li>\n<\/ul>\n","<ul>\n<li>Reduced replicability<\/li>\n<li>Reduced reproducibility<\/li>\n<li>Reduced validity of the measure<\/li>\n<li>Inflated or deflated reliability of the measure<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Inflated or deflated effect size estimates<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Reduced replicability<\/li>\n<li>Reduced reproducibility<\/li>\n<li>Compromised generalizability<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Compromised generalizability<\/li>\n<li>Reduced replicability<\/li>\n<li>Reduced reproducibility<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Reduced replicability<\/li>\n<li>Reduced reproducibility<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<\/ul>\n","<ul>\n<li>Inflated type I or type II error<\/li>\n<\/ul>\n","<ul>\n<li>Inflated type I or type II error<\/li>\n<li>Reduced replicability<\/li>\n<li>Reduced reproducibility<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Reduced replicability<\/li>\n<li>Reduced reproducibility<\/li>\n<\/ul>\n","<ul>\n<li>Inflated confidence in the research<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Reduced reproducibility<\/li>\n<li>Reduced replicability<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Compromised generalizability<\/li>\n<li>Reduced replicability<\/li>\n<li>Reduced reproducibility<\/li>\n<\/ul>\n","<ul>\n<li>Increasing the credibility of low-evidence research<\/li>\n<li>Inflated credibility of statements<\/li>\n<\/ul>\n","<ul>\n<li>Inflated confidence in the research<\/li>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Reduced replicability<\/li>\n<\/ul>\n","<ul>\n<li>Inflated type I or type II error<\/li>\n<li>Reduced reproducibility<\/li>\n<\/ul>\n","<p>—<\/p>\n","<ul>\n<li>Inflated confidence in the research<\/li>\n<li>Reduced replicability<\/li>\n<\/ul>\n","<ul>\n<li>Compromised generalizability<\/li>\n<li>Reduced replicability<\/li>\n<li>Reduced reproducibility<\/li>\n<\/ul>\n","<ul>\n<li>Inflated credibility of statements<\/li>\n<li>Inflated confidence in the research<\/li>\n<\/ul>\n","<ul>\n<li>Inflated confidence in the research<\/li>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Compromised generalizability<\/li>\n<li>Reduced replicability<\/li>\n<\/ul>\n","<ul>\n<li>Inflated or deflated effect size estimates<\/li>\n<li>Inflated type I or type II error<\/li>\n<li>Compromised generalizability<\/li>\n<li>Reduced replicability<\/li>\n<\/ul>\n","<ul>\n<li>Inflated credibility of statements<\/li>\n<\/ul>\n","<ul>\n<li>Inflated credibility of publications<\/li>\n<li>Inflated credibility of journals<\/li>\n<\/ul>\n","<p>—<\/p>\n","<ul>\n<li>Biased effect size estimates in meta-analyses (due to non-independence of results)<\/li>\n<li>Inflated type I or type II error due to unknown family-wise error rate<\/li>\n<\/ul>\n","<ul>\n<li>Lack of deserved credit for contributing authors<\/li>\n<li>Inflated confidence in the research based on the reputation of authors who were included in (or excluded from) the author list<\/li>\n<\/ul>\n","<p>—<\/p>\n","<ul>\n<li>Restricted potential for secondary data analysis<\/li>\n<li>Reduced reproducibility<\/li>\n<\/ul>\n","<ul>\n<li>Inflated confidence in a multi-study paper<\/li>\n<li>Publication bias<\/li>\n<\/ul>\n"],["<ul>\n<li>Run pilot studies to investigate if the manipulation can elicit an effect<\/li>\n<li>Use manipulation check questions<\/li>\n<li>Use standard stimuli or dosages<\/li>\n<\/ul>\n","<ul>\n<li>Frame questions in a neutral way<\/li>\n<li>Get external opinions on questionnaires/study materials OR conduct a registered report<\/li>\n<li>Use questionnaires that are unbiased and psychometrically sound<\/li>\n<\/ul>\n","<ul>\n<li>Be transparent about similar items<\/li>\n<li>Consider whether the measures used in a study distinctly operationalize different constructs<\/li>\n<li>Account for covariance due to similar items<\/li>\n<\/ul>\n","<ul>\n<li>Be transparent about when the preregistration was done<\/li>\n<li>Disclose the familiarity (if any) of the researcher with the data<\/li>\n<li>Preregister before analyzing the data<\/li>\n<\/ul>\n","<ul>\n<li>Be transparent about how power analysis was conducted<\/li>\n<li>Use meaningful parameters that are based on field standards, literature, or common sense<\/li>\n<\/ul>\n","<ul>\n<li>Avoid exposing participants to any cues that might influence their responses<\/li>\n<li>Blind the experimenter where possible<\/li>\n<li>Researchers interacting with participants should remain neutral and follow scripts during testing<\/li>\n<li>Use automated research procedures (e.g. research presentation software) instead of human research assistants wherever possible<\/li>\n<\/ul>\n","<ul>\n<li>Don't include pilot data in the analyzed dataset<\/li>\n<li>Report pilot data separately<\/li>\n<\/ul>\n","<ul>\n<li>Preregister stopping rules and adjustments for type I error-inflation<\/li>\n<li>Preregister the estimated sample size<\/li>\n<\/ul>\n","<ul>\n<li>Consider using statistical control for confounding variables<\/li>\n<li>Make sure that the sample represents the population<\/li>\n<li>Preregister the sampling process<\/li>\n<li>Report the sampling process transparently<\/li>\n<li>Use a sampling method that doesn't bias the results<\/li>\n<li>Use comparable (or matched) groups<\/li>\n<\/ul>\n","<ul>\n<li>Perform blinded data analysis<\/li>\n<li>Perform sensitivity analysis<\/li>\n<li>Preregister the study<\/li>\n<li>Use original measurement levels<\/li>\n<\/ul>\n","<ul>\n<li>Perform blinded data analysis<\/li>\n<li>Perform sensitivity analysis<\/li>\n<li>Preregister the study<\/li>\n<li>Report post hoc changes in exclusion criteria<\/li>\n<\/ul>\n","<ul>\n<li>Perform blinded data analysis<\/li>\n<li>Perform sensitivity analysis<\/li>\n<li>Preregister missing data approach<\/li>\n<\/ul>\n","<ul>\n<li>Describe and justify any modifications on measurements<\/li>\n<li>Perform blinded data analysis<\/li>\n<li>Perform sensitivity analysis<\/li>\n<li>Publish study materials<\/li>\n<li>Preregister the study<\/li>\n<li>Use conventional measurements/measures<\/li>\n<\/ul>\n","<ul>\n<li>Report post hoc changes in grouping rules and report results using original grouping rules as well<\/li>\n<li>Preregister the study<\/li>\n<li>Perform blinded data analysis<\/li>\n<li>Perform sensitivity analysis<\/li>\n<\/ul>\n","<ul>\n<li>Perform blinded data analysis<\/li>\n<li>Perform sensitivity analysis<\/li>\n<li>Preregister the study<\/li>\n<li>Report post hoc changes in exclusion criteria<\/li>\n<\/ul>\n","<ul>\n<li>Describe and justify any variable transformations<\/li>\n<li>Perform blinded data analysis<\/li>\n<li>Perform sensitivity analysis<\/li>\n<li>Preregister conditional transformations<\/li>\n<\/ul>\n","<ul>\n<li>Perform blinded data analysis<\/li>\n<li>Perform sensitivity analysis<\/li>\n<li>Use a theoretically justified model in confirmatory studies<\/li>\n<li>Underfitting: Visualize data and the model<\/li>\n<li>Use methods that prevent overfitting in exploratory research, e.g. use separate train and test datasets, use cross-validation resampling methods, use regularization or other feature selection methods<\/li>\n<\/ul>\n","<ul>\n<li>Perform blinded data analysis<\/li>\n<li>Perform sensitivity analysis<\/li>\n<li>Preregister p-value adjustment plans<\/li>\n<li>Preregister planned contrasts<\/li>\n<\/ul>\n","<ul>\n<li>Perform and report necessary assumption checks<\/li>\n<\/ul>\n","<ul>\n<li>Only report significance if the results are robust across random seeds<\/li>\n<li>Use a large number of replications (e.g., bootstrap samples)<\/li>\n<li>Perform blinded data analysis<\/li>\n<\/ul>\n","<ul>\n<li>Perform blinded data analysis<\/li>\n<li>Perform specificity curve analysis<\/li>\n<li>Preregister data processing (e.g., missing data approach) and statistical analysis strategy<\/li>\n<li>Report all performed hypothesis-tests<\/li>\n<\/ul>\n","<ul>\n<li>Perform blinded data analysis<\/li>\n<li>Preregister complete models/analytical plan<\/li>\n<li>Report robustness checks<\/li>\n<\/ul>\n","<ul>\n<li>Never cite a retracted study<\/li>\n<li>Only cite publications that properly support their claims<\/li>\n<\/ul>\n","<ul>\n<li>Clearly separate exploratory and confirmatory findings<\/li>\n<li>Form hypotheses before analyzing the data<\/li>\n<li>Perform blinded data analysis<\/li>\n<li>Preregister confirmatory hypotheses<\/li>\n<li>Use robust exploratory research practices (e.g. holdout dataset, cross-validation, multiverse analysis, blinded data analysis, etc.)<\/li>\n<\/ul>\n","<ul>\n<li>Adhere to reporting conventions (e.g., APA)<\/li>\n<li>Publish data<\/li>\n<li>Publish processing and analysis code<\/li>\n<li>Use literate programming (e.g., RMarkdown, quarto, jupyter)<\/li>\n<li>Work with software that supports you in producing and checking your write-up (e.g., papaja, stat-check)<\/li>\n<\/ul>\n","<ul>\n<li>Make it clear that the evidence is limited to certain contexts<\/li>\n<li>Make sure that every interpretation is properly supported by evidence<\/li>\n<li>Use conditional statements where evidence is weak or the researcher uses extrapolation<\/li>\n<\/ul>\n","<ul>\n<li>Avoid vagueness in preregistration<\/li>\n<li>Disclose and justify every divergence from the preregistration<\/li>\n<li>Use methods that will provide robust results even when preregistration is not specific at points (e.g. blind data analysis, cross-validation)<\/li>\n<\/ul>\n","<ul>\n<li>Preregister the study or written research plan before conducting the study<\/li>\n<li>Report every important detail of the scientific process<\/li>\n<li>Use a lab log during data collection to keep track of changes in the scientific process<\/li>\n<\/ul>\n","<ul>\n<li>Provide comprehensive coverage of related scholarly literature<\/li>\n<\/ul>\n","<ul>\n<li>If some hypotheses get left out due to the scope of the write-up be transparent about it<\/li>\n<li>Preregister the study<\/li>\n<li>Report all hypotheses in the write-up regardless of whether they were confirmed or not<\/li>\n<\/ul>\n","<ul>\n<li>Perform blinded data analysis<\/li>\n<li>Preregister the study<\/li>\n<li>Report all indicators<\/li>\n<\/ul>\n","<ul>\n<li>Perform blinded data analysis<\/li>\n<li>Preregister the study<\/li>\n<li>Report all outcomes<\/li>\n<\/ul>\n","<ul>\n<li>Cite only relevant studies<\/li>\n<\/ul>\n","<ul>\n<li>Cite only relevant studies<\/li>\n<li>Provide comprehensive coverage of related scholarly literature<\/li>\n<\/ul>\n","<ul>\n<li>Follow best practices on how to visualize data<\/li>\n<\/ul>\n","<ul>\n<li>Preregister the publication strategy<\/li>\n<li>Publish study results in one single publication or disclose all related papers<\/li>\n<\/ul>\n","<ul>\n<li>Explicitly declare contributions to the project (e.g., CRediT taxonomy)<\/li>\n<li>Include everyone who made a significant contribution to the project<\/li>\n<li>Only include authors who contributed to the project<\/li>\n<\/ul>\n","<ul>\n<li>Always link the preregistration to the manuscript and report discrepancies<\/li>\n<\/ul>\n","<ul>\n<li>Data should be shared based on the FAIR (findable, accessible, interoperable, and reusable) principles and legislative context of the researcher<\/li>\n<li>If confidential and personal information makes participants identifiable, apply masking and anonymization, and then share data<\/li>\n<li>Use synthetic datasets when original data can't be shared<\/li>\n<\/ul>\n","<ul>\n<li>Only preregister on platforms that will eventually publish all preregistrations<\/li>\n<li>Publish all studies, even when the findings do not support hypotheses<\/li>\n<\/ul>\n"],["<p>Maybe<\/p>\n","<p>Yes<\/p>\n","<p>Yes<\/p>\n","<p>Yes<\/p>\n","<p>Yes<\/p>\n","<p>No<\/p>\n","<p>Maybe<\/p>\n","<p>Maybe<\/p>\n","<p>Yes<\/p>\n","<p>Yes<\/p>\n","<p>Maybe<\/p>\n","<p>Maybe<\/p>\n","<p>Maybe<\/p>\n","<p>Maybe<\/p>\n","<p>Maybe<\/p>\n","<p>Maybe<\/p>\n","<p>Yes<\/p>\n","<p>Yes<\/p>\n","<p>Yes<\/p>\n","<p>Maybe<\/p>\n","<p>Maybe<\/p>\n","<p>Maybe<\/p>\n","<p>Yes<\/p>\n","<p>Maybe<\/p>\n","<p>Yes<\/p>\n","<p>Yes<\/p>\n","<p>Yes<\/p>\n","<p>Maybe<\/p>\n","<p>Yes<\/p>\n","<p>Maybe<\/p>\n","<p>Maybe<\/p>\n","<p>Maybe<\/p>\n","<p>Yes<\/p>\n","<p>Yes<\/p>\n","<p>Yes<\/p>\n","<p>Maybe<\/p>\n","<p>No<\/p>\n","<p>Maybe<\/p>\n","<p>Yes<\/p>\n","<p>No<\/p>\n"],["<ul>\n<li>Lack of manipulation check<\/li>\n<li>Using dosages outside of recommended values<\/li>\n<li>Using stimuli that can elicit extreme responses<\/li>\n<li>Using stimuli that were not previously tested and/or have no proven effect<\/li>\n<\/ul>\n","<ul>\n<li>Ad hoc questionnaires are used instead of validated instruments<\/li>\n<li>Measurement items use suggestive or biased language<\/li>\n<li>Measurement is based on single-item questions<\/li>\n<li>No discussion of the psychometric properties of the instruments<\/li>\n<\/ul>\n","<ul>\n<li>Items or tasks are similar for the correlated constructs<\/li>\n<\/ul>\n","<ul>\n<li>Data collection occurred previous to the preregistration date<\/li>\n<li>Date of preregistration is unrealistically close to the first submission without the paper being a registered report<\/li>\n<\/ul>\n","<ul>\n<li>Details of power analysis not reported or justified<\/li>\n<li>Parameters of the power analysis are generic and do not fit the study<\/li>\n<li>Relatively low sample size<\/li>\n<li>The results of power analysis are misinterpreted<\/li>\n<\/ul>\n","<ul>\n<li>Absence of blinding<\/li>\n<li>Absence of procedure scripts<\/li>\n<li>Suggestive questions in the survey<\/li>\n<\/ul>\n","<ul>\n<li>If data are shared, timestamps may fall into two distinct periods<\/li>\n<li>Sample sizes reported throughout the paper might not match<\/li>\n<\/ul>\n","<ul>\n<li>Absence of preregistration<\/li>\n<li>Low sample size<\/li>\n<li>P-values are just below the significance threshold (usually 0.05)<\/li>\n<li>Relatively large effect size compared to other studies in the field<\/li>\n<li>Vague or absent reason for sample size<\/li>\n<\/ul>\n","<ul>\n<li>Compared groups are from different populations<\/li>\n<li>Convenience sample<\/li>\n<li>Unclear rationale for sample selection<\/li>\n<\/ul>\n","<ul>\n<li>Absence of open data<\/li>\n<li>Scale or response options in materials or methods do not match how they are reported in the results<\/li>\n<li>Test statistics do not match expected data analysis strategy<\/li>\n<\/ul>\n","<ul>\n<li>Absence of open data<\/li>\n<li>Absence of preregistration<\/li>\n<li>Unexplained discrepancy between the recruited and analyzed sample sizes and degrees of freedom<\/li>\n<\/ul>\n","<ul>\n<li>Absence of open data<\/li>\n<li>Lack of rationale or references supporting the missing data approach<\/li>\n<li>No mention of the missing data approach or the missing data at all<\/li>\n<li>Unexplained discrepancy between the recruited and analyzed sample sizes<\/li>\n<\/ul>\n","<ul>\n<li>Absence of open data<\/li>\n<li>Absence of open study materials<\/li>\n<li>Discrepancy between the reported version of measurement/measure and original or conventional measure<\/li>\n<\/ul>\n","<ul>\n<li>Absence of open data<\/li>\n<li>Absence of preregistration<\/li>\n<li>Oversimplified sample description<\/li>\n<li>Response options in materials/methods different than reported groups<\/li>\n<\/ul>\n","<ul>\n<li>Absence of open data<\/li>\n<li>Absence of preregistration<\/li>\n<li>Sample too narrow for recruitment methods<\/li>\n<li>Unexplained discrepancy between the recruited and analyzed sample sizes<\/li>\n<\/ul>\n","<ul>\n<li>Abscence of open data<\/li>\n<li>Reported values are outside of regular range<\/li>\n<li>Transformation is applied without justification<\/li>\n<li>Using transformations that are unconventional for the measure<\/li>\n<\/ul>\n","<ul>\n<li>Improper prediction selection (e.g., no regularization)<\/li>\n<li>No mention of holdout (or test) dataset or cross-validation<\/li>\n<li>Overfitting: Very high R2 value (close to 1)<\/li>\n<li>The number of included predictors in the model is large<\/li>\n<li>The number of observations is low<\/li>\n<li>Underfitting: Data visualization shows high model bias<\/li>\n<\/ul>\n","<ul>\n<li>Multiple tests are made that would require p-value adjustment<\/li>\n<li>P-value adjustment not mentioned<\/li>\n<li>Using p-value correction that is too strict (e.g., Bonferroni) without proper justification<\/li>\n<\/ul>\n","<ul>\n<li>Evidence of assumption breaches (e.g., non-normality, non-independent data, largely different SDs by group)<\/li>\n<li>Not reporting assumption checks<\/li>\n<\/ul>\n","<ul>\n<li>P-values just below the significance threshold (usually 0.05)<\/li>\n<\/ul>\n","<ul>\n<li>Absence of preregistration<\/li>\n<li>Arbitrary data processing steps and/or statistical methods<\/li>\n<li>P-values just below the significance threshold (usually 0.05)<\/li>\n<\/ul>\n","<ul>\n<li>Different covariates in different analysis steps are used<\/li>\n<li>There is a lack of justification for the selection of the covariates<\/li>\n<\/ul>\n","<ul>\n<li>The cited publication provides no or low-quality evidence to its claims<\/li>\n<li>The cited publication is retracted or otherwise discredited or its claims are refuted<\/li>\n<\/ul>\n","<ul>\n<li>Absence of preregistration<\/li>\n<li>Unexplained and unconventional choices in the methods and results section<\/li>\n<\/ul>\n","<ul>\n<li>Absence of open code<\/li>\n<li>Absence of open data<\/li>\n<li>Anomalies in reported statistics, e.g., test statistics are incompatible with p-values<\/li>\n<li>Fit statistics are missing without proper explanation<\/li>\n<li>Statistics are not reported according to conventions (e.g., three digits for p-values, reporting of df)<\/li>\n<\/ul>\n","<ul>\n<li>Causal claims are made without the methodology or analysis allowing causal inference<\/li>\n<li>Results are generalized to contexts outside of the study's scope<\/li>\n<li>The chosen methodology and statistical analysis do not allow to answer the hypothesis<\/li>\n<li>The statistical results do not match the conclusions<\/li>\n<\/ul>\n","<ul>\n<li>Link to the preregistration in the manuscript does not work or leads to a page that cannot be accessed<\/li>\n<li>Preregistration and published study differ on important aspects<\/li>\n<\/ul>\n","<ul>\n<li>Absence of open study materials<\/li>\n<li>Details that are usually shared are missing<\/li>\n<li>Replication is not possible from published methods<\/li>\n<\/ul>\n","<ul>\n<li>Cited studies only point into one direction<\/li>\n<li>Important studies and experts are missing from references<\/li>\n<li>Systematic reviews and meta-analyses are not cited<\/li>\n<\/ul>\n","<ul>\n<li>Number of hypotheses in preregistration (or dissertation) exceeds the number in the publication<\/li>\n<\/ul>\n","<ul>\n<li>Indicators reported in Supplemental Material but not mentioned in main text<\/li>\n<li>Measures get reported in the methods section but not in the results section<\/li>\n<li>Number of preregistered indicators exceeds the number of indicators in publication<\/li>\n<li>Reported mean time of participation does not match the number of reported measures<\/li>\n<\/ul>\n","<ul>\n<li>Measures get reported in the methods section but not in the results section<\/li>\n<li>Number of preregistered outcomes exceeds number outcomes in publication<\/li>\n<li>Outcomes reported in Supplemental Material but not mentioned in main text<\/li>\n<li>Reported mean time of participation does not match number of reported measures<\/li>\n<\/ul>\n","<ul>\n<li>Publications are cited without relevance to the claims<\/li>\n<\/ul>\n","<ul>\n<li>Publications are cited without relevance to the claims<\/li>\n<li>Specific authors or journals are cited disproportionately frequently<\/li>\n<\/ul>\n","<ul>\n<li>Chartjunk (e.g., 3D elements, ornaments) is present on the plot<\/li>\n<li>In a plot y-axis is starting at an arbitrary point<\/li>\n<li>Only summary statistics are shown without individual data points<\/li>\n<li>Scale or response options in text do not match how they are presented in a plot<\/li>\n<li>Statistical uncertainty (e.g., error bars) is not shown on plots<\/li>\n<li>Visualization does not match the reported results in the text<\/li>\n<\/ul>\n","<ul>\n<li>Absence of open data<\/li>\n<li>Description of the sample is the same over several studies by the same researcher or lab<\/li>\n<li>Several papers exist with similar outcomes or predictors based on the same dataset by the same researcher or lab<\/li>\n<li>The methods suggest a large study but the scope of the paper is narrow<\/li>\n<\/ul>\n","<p>None<\/p>\n","<ul>\n<li>A preregistration that fits the study is findable<\/li>\n<\/ul>\n","<ul>\n<li>Data are not shared according to FAIR principles<\/li>\n<li>No information in the publication on the availability of the data<\/li>\n<\/ul>\n","<ul>\n<li>Publication bias can be estimated in meta-analysis<\/li>\n<\/ul>\n"],["<ul>\n<li>Joe, S., &amp; Leif, N. (2020, March 10). Data Replicada #4: The Problem of Hidden Confounds. Data Colada. https://datacolada.org/85<\/li>\n<li>Marchetti, S., &amp; Schellens, J. H. M. (2007). The impact of FDA and EMEA guidelines on drug development in relation to Phase 0 trials. British Journal of Cancer, 97(5), 577–581. https://doi.org/10.1038/sj.bjc.6603925<\/li>\n<\/ul>\n","<ul>\n<li>Flake, J. K., &amp; Fried, E. I. (2020). Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them. Advances in Methods and Practices in Psychological Science, 3(4), 456–465. https://doi.org/10.1177/2515245920952393<\/li>\n<\/ul>\n","<ul>\n<li>Flake, J. K., &amp; Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. Advances in Methods and Practices in Psychological Science, 3(4), 456-465. https://doi.org/10.1177/2515245920952393<\/li>\n<li>Hodson, G. (2021). Construct jangle or construct mangle? Thinking straight about (nonredundant) psychological constructs. Journal of Theoretical Social Psychology, 5(4), 576–590. https://doi.org/10.1002/jts5.120<\/li>\n<li>Wang, Y. A., &amp; Eastwick, P. W. (2020). Solutions to the problems of incremental validity testing in relationship science. Personal Relationships, 27(1), 156-175. https://doi.org/10.1111/pere.12309<\/li>\n<\/ul>\n","<ul>\n<li>Yamada, Y. (2018). How to crack pre-registration: toward transparent and Open Science. Frontiers in Psychology, 9, 1831. https://doi.org/10.3389/fpsyg.2018.01831<\/li>\n<\/ul>\n","<ul>\n<li>Heckman, M. G., Davis, J. M., 3rd, &amp; Crowson, C. S. (2022). Post Hoc Power Calculations: An Inappropriate Method for Interpreting the Findings of a Research Study. The Journal of Rheumatology, 49(8), 867–870. https://doi.org/10.3899/jrheum.211115<\/li>\n<li>Kovacs, M., van Ravenzwaaij, D., Hoekstra, R., &amp; Aczel, B. (2022). SampleSizePlanner: A Tool to Estimate and Justify Sample Size for Two-Group Studies. Advances in Methods and Practices in Psychological Science, 5(1), 25152459211054059. https://doi.org/10.1177/25152459211054059<\/li>\n<li>Lakens, D. (2022). Sample size justification. Collabra. Psychology, 8(1). https://doi.org/10.1525/collabra.33267<\/li>\n<\/ul>\n","<ul>\n<li>McCambridge, J., de Bruin, M., &amp; Witton, J. (2012). The effects of demand characteristics on research participant behaviours in non-laboratory settings: a systematic review. PloS One, 7(6), e39116. https://doi.org/10.1371/journal.pone.0039116<\/li>\n<\/ul>\n","<ul>\n<li>Kravitz, D., &amp; Mitroff, S. (2020). Quantifying, and correcting for, the impact of questionable research practices on false discovery rates in psychological science. https://doi.org/10.31234/osf.io/fu9gy<\/li>\n<li>Kriegeskorte, N., Simmons, W. K., Bellgowan, P. S. F., &amp; Baker, C. I. (2010). Circular analysis in systems neuroscience the dangers of double dipping. Nature Neuroscience, 12(5), 535–540. https://doi.org/10.1038/nn.2303<\/li>\n<\/ul>\n","<ul>\n<li>de Heide, R., &amp; Grünwald, P. D. (2021). Why optional stopping can be a problem for Bayesians. Psychonomic Bulletin &amp; Review, 28(3), 795–812. https://doi.org/10.3758/s13423-020-01803-x<\/li>\n<li>Lakens, D. (2022). Sample size justification. Collabra. Psychology, 8(1). https://doi.org/10.1525/collabra.33267<\/li>\n<li>Schönbrodt, F. D., &amp; Perugini, M. (2013). At what sample size do correlations stabilize? Journal of Research in Personality, 47(5), 609–612. https://doi.org/10.1016/j.jrp.2013.05.009<\/li>\n<li>Schönbrodt, F. D., Wagenmakers, E.-J., Zehetleitner, M., &amp; Perugini, M. (2017). Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences. Psychological Methods, 22, 322–339. doi:10.1037/met0000061 [OSF project with reproducible code, workshop slides, presentation slides]<\/li>\n<li>Wicherts, J. M., Veldkamp, C. L. S., Augusteijn, H. E. M., Bakker, M., van Aert, R. C. M., &amp; van Assen, M. A. L. M. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. Frontiers in Psychology, 7, 1832. https://doi.org/10.3389/fpsyg.2016.01832<\/li>\n<\/ul>\n","<ul>\n<li>DeepChecks. (n.d.). Selective Sampling. deepchecks.com. Retrieved May 15, 2023, from https://deepchecks.com/glossary/selective-sampling/<\/li>\n<li>Marks, E. S. (1947). Selective sampling in psychological research. Psychological Bulletin, 44(3), 267–275. https://doi.org/10.1037/h0061812<\/li>\n<\/ul>\n","<ul>\n<li>Cohen, J. (1983). The cost of dichotomization. Applied Psychological Measurement, 7, 247-253. https://doi.org/10.1177/014662168300700301<\/li>\n<li>DeCoster, J., Gallucci, M., &amp; Iselin, A.-M. R. (2011). Best practices for using median splits, artificial categorization, and their continuous alternatives. Journal of Experimental Psychopathology, 2(2), 197–209. https://doi.org/10.5127/jep.008310<\/li>\n<li>MacCallum, R. C., Zhang, S., Preacher, K. J., &amp; Rucker, D. D. (2002). On the practice of dichotomization of quantitative variables. Psychological Methods, 7(1), 19–40. https://doi.org/10.1037/1082-989X.7.1.19<\/li>\n<\/ul>\n","<ul>\n<li>Bakker, M., &amp; Wicherts, J. M. (2014). Outlier removal and the relation with reporting errors and quality of psychological research. PloS one, 9(7), e103360. https://doi.org/10.1371/journal.pone.0103360<\/li>\n<li>Bakker, M., &amp; Wicherts, J. M. (2014). Outlier removal, sum scores, and the inflation of the Type I error rate in independent samples t tests: the power of alternatives and recommendations. Psychological Methods, 19(3), 409–427. https://doi.org/10.1037/met0000014<\/li>\n<li>Osborne, J. W., &amp; Overbay, A. (2004). The power of outliers (and why researchers should always check for them). Practical Assessment, Research, and Evaluation, 9(1), 6. https://doi.org/10.7275/qf69-7k43<\/li>\n<\/ul>\n","<ul>\n<li>Enders, C. K. (2010). Applied missing data analysis. Guilford Press.<\/li>\n<li>Woods, A. D., Gerasimova, D., Van Dusen, B., Nissen, J., Bainter, S., Uzdavines, A., Davis-Kean, P. E., Halvorson, M., King, K. M., Logan, J. A. R., Xu, M., Vasilev, M. R., Clay, J. M., Moreau, D., Joyal-Desmarais, K., Cruz, R. A., Brown, D. M. Y., Schmidt, K., &amp; Elsherif, M. M. (2023). Best practices for addressing missing data through multiple imputation. Infant and Child Development. https://doi.org/10.1002/icd.2407<\/li>\n<\/ul>\n","<ul>\n<li>Flake, J. K., &amp; Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. Advances in Methods and Practices in Psychological Science, 3(4), 456-465. https://doi.org/10.1177/2515245920952393<\/li>\n<\/ul>\n","<ul>\n<li>Wicherts, J. M., Veldkamp, C. L., Augusteijn, H. E., Bakker, M., Van Aert, R., &amp; Van Assen, M. A. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. Frontiers in Psychology, 1832. https://doi.org/10.3389/fpsyg.2016.0183<\/li>\n<\/ul>\n","<ul>\n<li>Lang, S., Armstrong, N., Deshpande, S., Ramaekers, B., Grimm, S., de Kock, S., Kleijnen, J., &amp; Westwood, M. (2019). Clinically inappropriate post hoc exclusion of study participants from test accuracy calculations: the ROMA score, an example from a recent NICE diagnostic assessment. Annals of clinical biochemistry, 56(1), 72–81. https://doi.org/10.1177/0004563218782722<\/li>\n<li>Nüesch, E., Trelle, S., Reichenbach, S., Rutjes, A. W. S., Bürgi, E., Scherer, M., Altman, D. G., &amp; Jüni, P. (2009). The effects of excluding patients from the analysis in randomised controlled trials: meta-epidemiological study. BMJ (Clinical Research Ed.), 339(sep07 1), b3244. https://doi.org/10.1136/bmj.b3244<\/li>\n<\/ul>\n","<ul>\n<li>Lee, D. K. (2020). Data transformation: a focus on the interpretation. Korean Journal of Anesthesiology, 73(6), 503–508. https://doi.org/10.4097/kja.20137<\/li>\n<\/ul>\n","<ul>\n<li>Babyak, M. A. (2004). What you see may not be what you get: a brief, nontechnical introduction to overfitting in regression-type models. Psychosomatic medicine, 66(3), 411-421.<\/li>\n<\/ul>\n","<ul>\n<li>Bender, R., &amp; Lange, S. (2001). Adjusting for multiple testing—when and how?. Journal of clinical epidemiology, 54(4), 343-349. https://doi.org/10.1016/S0895-4356(00)00314-0<\/li>\n<li>Cramer, A. O., van Ravenzwaaij, D., Matzke, D., Steingroever, H., Wetzels, R., Grasman, R. P., ... &amp; Wagenmakers, E. J. (2016). Hidden multiplicity in exploratory multiway ANOVA: Prevalence and remedies. Psychonomic bulletin &amp; review, 23(2), 640-647.  https://doi.org/10.3758/s13423-015-0913-5<\/li>\n<li>Midway, S., Robertson, M., Flinn, S., &amp; Kaller, M. (2020). Comparing multiple comparisons: practical guidance for choosing the best multiple comparisons test. PeerJ, 8, e10387. https://doi.org/10.7717/peerj.10387.<\/li>\n<\/ul>\n","<ul>\n<li>Knief, U., &amp; Forstmeier, W. (2021). Violating the normality assumption may be the lesser of two evils. Behavior Research Methods, 53(6), 2576-2590. https://doi.org/10.3758/s13428-021-01587-5<\/li>\n<li>Meteyard, L., &amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092<\/li>\n<\/ul>\n","<ul>\n<li>Götz, M., O'Boyle, E. H., Gonzalez-Mulé, E., Banks, G. C., &amp; Bollmann, S. S. (2021). The “Goldilocks Zone”: (Too) many confidence intervals in tests of mediation just exclude zero. Psychological Bulletin, 147(1), 95–114. https://doi.org/10.1037/bul0000315<\/li>\n<li>Stack Exchange. (2018). Choosing the &quot;correct&quot; seed for reproducible research/results. Cross Validated. Retrieved January 10, 2023, from https://stats.stackexchange.com/questions/335936/choosing-the-correct-seed-for-reproducible-research-results<\/li>\n<\/ul>\n","<ul>\n<li>Chuard, P. J. C., Vrtílek, M., Head, M. L., &amp; Jennions, M. D. (2019). Evidence that nonsignificant results are sometimes preferred: Reverse P-hacking or selective reporting? PLoS Biology, 17(1), e3000127. https://doi.org/10.1371/journal.pbio.3000127<\/li>\n<li>Head, M. L., Holman, L., Lanfear, R., Kahn, A. T., &amp; Jennions, M. D. (2015). The extent and consequences of p-hacking in science. PLoS Biology, 13(3), e1002106. https://doi.org/10.1371/journal.pbio.1002106<\/li>\n<li>Olejnik, S. F., &amp; Algina, J. (1987). Type I error rates and power estimates of selected parametric and nonparametric tests of scale. Journal of Educational Statistics, 12(1), 45-61.  https://doi.org/10.3102/10769986012001045<\/li>\n<li>Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632<\/li>\n<li>Wagenmakers, EJ. (2007). A practical solution to the pervasive problems of p values. Psychonomic Bulletin &amp; Review 14, 779–804. https://doi.org/10.3758/BF03194105<\/li>\n<\/ul>\n","<ul>\n<li>Becker, T. E., Atinc, G., Breaugh, J. A., Carlson, K. D., Edwards, J. R., &amp; Spector, P. E. (2016). Statistical control in correlational studies: 10 essential recommendations for organizational researchers. Journal of Organizational Behavior, 37(2), 157-167. https://doi.org/10.1002/job.2053<\/li>\n<li>Stefan, A., &amp; Schönbrodt, F. D. (2022, March 16). Big little lies: A compendium and simulation of p-hacking strategies. https://doi.org/10.31234/osf.io/xy2dk<\/li>\n<li>VanderWeele, T. J. (2019). Principles of confounder selection. European journal of epidemiology, 34(3), 211-219. https://doi.org/10.1007/s10654-019-00494-6<\/li>\n<li>Wysocki, A. C., Lawson, K. M., &amp; Rhemtulla, M. (2022). Statistical control requires causal justification. Advances in Methods and Practices in Psychological Science, 5(2), 25152459221095823. https://doi.org/10.1177/25152459221095823.<\/li>\n<\/ul>\n","<ul>\n<li>American Psychological Association. (2023, November). Journal Article Reporting Standards (JARS). https://apastyle.apa.org/jars<\/li>\n<li>Balshem, H., Helfand, M., Schünemann, H. J., Oxman, A. D., Kunz, R., Brozek, J., Vist, G. E., Falck-Ytter, Y., Meerpohl, J., Norris, S., &amp; Guyatt, G. H. (2011). GRADE guidelines: 3. Rating the quality of evidence. Journal of Clinical Epidemiology, 64(4), 401–406. https://doi.org/10.1016/j.jclinepi.2010.07.015<\/li>\n<li>Letrud, K., &amp; Hernes, S. (2019). Affirmative citation bias in scientific myth debunking: A three-in-one case study. PLOS ONE, 14(9), e0222213. https://doi.org/10.1371/journal.pone.0222213<\/li>\n<\/ul>\n","<ul>\n<li>Andrade, C. (2021). HARKing, Cherry-Picking, P-Hacking, Fishing Expeditions, and Data Dredging and Mining as Questionable Research Practices. The Journal of Clinical Psychiatry, 82(1). https://doi.org/10.4088/JCP.20f13804<\/li>\n<li>Brookes, S.T., Whitley, E., Peters, T.J., Mulheran, P.A., Egger, M. &amp; Davey Smith, G. (2001). Subgroup analysis in randomised controlled trials: Quantifying the risks of false-positives and false-negatives, Health Technology Assessment, 5(33), 1–56. https://doi.org/10.3310/hta5330<\/li>\n<li>Kerr, N. L. (1998). HARKing: hypothesizing after the results are known. Personality and Social Psychology Review: An Official Journal of the Society for Personality and Social Psychology, Inc, 2(3), 196–217. https://doi.org/10.1207/s15327957pspr0203_4<\/li>\n<li>Leung, K. (2011). Presenting post hoc hypotheses as a priori: Ethical and theoretical issues. Management and Organization Review, 7(3), 471–479. https://doi.org/10.1111/j.1740-8784.2011.00222.x<\/li>\n<li>Weston, S. J., Ritchie, S. J., Rohrer, J. M., &amp; Przybylski, A. K. (2019). Recommendations for increasing the transparency of analysis of preexisting data sets. Advances in Methods and Practices in Psychological Science, 2(3), 214–227. https://doi.org/10.1177/2515245919848684<\/li>\n<\/ul>\n","<ul>\n<li>Bakker, M., &amp; Wicherts, J. M. (2011). The (mis)reporting of statistical results in psychology journals. Behavior Research Methods, 43(3), 666–678. https://doi.org/10.3758/s13428-011-0089-5<\/li>\n<li>Jackson, D. L., Gillaspy, J. A., &amp; Purc-Stephenson, R. (2009). Reporting practices in confirmatory factor analysis: an overview and some recommendations. Psychological Methods, 14(1), 6–23. https://doi.org/10.1037/a0014694<\/li>\n<li>Nuijten, Michèle B., Hartgerink, C. H. J., van Assen, M. A. L. M., Epskamp, S., &amp; Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985–2013). Behavior Research Methods, 48(4), 1205–1226. https://doi.org/10.3758/s13428-015-0664-2<\/li>\n<li>Nuijten, Michele B., van Assen, M. A. L. M., Hartgerink, C. H. J., Epskamp, S., &amp; Wicherts, J. M. (2017). The validity of the tool “statcheck” in discovering statistical reporting inconsistencies. In PsyArXiv. https://doi.org/10.31234/osf.io/tcxaj<\/li>\n<\/ul>\n","<ul>\n<li>Simons, D. J., Shoda, Y., &amp; Lindsay, D. S. (2017). Constraints on Generality (COG): A proposed addition to all empirical papers. Perspectives on Psychological Science: A Journal of the Association for Psychological Science, 12(6), 1123–1128. https://doi.org/10.1177/1745691617708630<\/li>\n<li>Yarkoni, T. (2020). The generalizability crisis. The Behavioral and Brain Sciences, 45, e1. https://doi.org/10.1017/S0140525X20001685<\/li>\n<\/ul>\n","<ul>\n<li>Adam, D. (2019). A solution to psychology’s reproducibility problem just failed its first test. Science. https://doi.org/10.1126/science.aay1207<\/li>\n<li>Claesen, A., Gomes, S., Tuerlinckx, F., &amp; Vanpaemel, W. (2021). Comparing dream to reality: an assessment of adherence of the first generation of preregistered studies. Royal Society Open Science, 8(10), 211037. https://doi.org/10.1098/rsos.211037<\/li>\n<li>Nosek, B. A., Ebersole, C. R., DeHaven, A. C., &amp; Mellor, D. T. (2018). The pre-registration revolution. Proceedings of the National Academy of Sciences of the United States of America, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114<\/li>\n<\/ul>\n","<ul>\n<li>Gernsbacher, M. A. (2018). Writing Empirical Articles: Transparency, Reproducibility, Clarity, and Memorability. Advances in Methods and Practices in Psychological Science, 1(3), 403–414. https://doi.org/10.1177/2515245918754485<\/li>\n<li>National Academies of Sciences, Engineering, and Medicine, Policy and Global Affairs, Committee on Science, Engineering, Medicine, and Public Policy, Board on Research Data and Information, Division on Engineering and Physical Sciences, Committee on Applied and Theoretical Statistics, Board on Mathematical Sciences and Analytics, Division on Earth and Life Studies, Nuclear and Radiation Studies Board, &amp; Division of Behavioral and Social Sciences and Education. (2019). Reproducibility and Replicability in Science. National Academies Press.<\/li>\n<li>Wagenmakers, E.-J., Sarafoglou, A., Aarts, S., Albers, C., Algermissen, J., Bahník, Š., van Dongen, N., Hoekstra, R., Moreau, D., van Ravenzwaaij, D., Sluga, A., Stanke, F., Tendeiro, J., &amp; Aczel, B. (2021). Seven steps toward more transparency in statistical practice. Nature Human Behaviour, 5(11), 1473–1480. https://doi.org/10.1038/s41562-021-01211-8<\/li>\n<\/ul>\n","<ul>\n<li>Duyx, B., Urlings, M. J. E., Swaen, G. M. H., Bouter, L. M., &amp; Zeegers, M. P. (2017a). Scientific citations favor positive results: a systematic review and meta-analysis. Journal of Clinical Epidemiology, 88, 92–101. https://doi.org/10.1016/j.jclinepi.2017.06.002<\/li>\n<li>Gøtzsche, P. C. (2022). Citation bias: Questionable research practice or scientific misconduct? Journal of the Royal Society of Medicine, 115(1), 31–35. https://doi.org/10.1177/01410768221075881<\/li>\n<\/ul>\n","<ul>\n<li>Andrade C (2021). HARKing, cherry-picking, P-hacking, fishing expeditions, and data dredging and mining as questionable research practices. Journal of Clinical Psychiatry , 82(1), 20f13804. https://doi.org/10.4088/JCP.20f13804<\/li>\n<li>O’Boyle, E. H., Jr, Banks, G. C., &amp; Gonzalez-Mulé, E. (2017). The Chrysalis Effect: How ugly initial results metamorphosize into beautiful articles. Journal of Management, 43(2), 376–399. https://doi.org/10.1177/0149206314527133<\/li>\n<li>Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632<\/li>\n<\/ul>\n","<ul>\n<li>Gernsbacher, M. A. (2018). Writing empirical articles: Transparency, reproducibility, clarity, and memorability. Advances in Methods and Practices in Psychological Science, 1(3), 403–414. https://doi.org/10.1177/2515245918754485<\/li>\n<li>Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632<\/li>\n<\/ul>\n","<ul>\n<li>Gernsbacher, M. A. (2018). Writing Empirical Articles: Transparency, Reproducibility, Clarity, and Memorability. Advances in Methods and Practices in Psychological Science, 1(3), 403–414. https://doi.org/10.1177/2515245918754485<\/li>\n<li>Pigott, T. D., Valentine, J. C., Polanin, J. R., Williams, R. T., &amp; Canada, D. D. (2013). Outcome-reporting bias in education research. Educational Researcher, 42(8), 424–432. https://doi.org/10.3102/0013189X13507104<\/li>\n<li>Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632<\/li>\n<\/ul>\n","<ul>\n<li>Penders, B. (2018). Ten simple rules for responsible referencing. PLoS Computational Biology, 14(4), e1006036. https://doi.org/10.1371/journal.pcbi.1006036<\/li>\n<li>Teixeira da Silva, J. A., &amp; Vuong, Q.-H. (2021). The right to refuse unwanted citations: rethinking the culture of science around the citation. Scientometrics, 126(6), 5355–5360. https://doi.org/10.1007/s11192-021-03960-9<\/li>\n<\/ul>\n","<ul>\n<li>Fong, E. A., &amp; Wilhite, A. W. (2017). Authorship and citation manipulation in academic research. PloS One, 12(12), e0187394. https://doi.org/10.1371/journal.pone.0187394<\/li>\n<li>Mehregan, M. (2022). Scientific journals must be alert to potential manipulation in citations and referencing. Research Ethics, 18(2), 163–168. https://doi.org/10.1177/17470161211068745<\/li>\n<\/ul>\n","<ul>\n<li>Nguyen, V. T., Jung, K., &amp; Gupta, V. (2021). Examining data visualization pitfalls in scientific publications. Visual Computing for Industry, Biomedicine, and Art, 4(1), 27. https://doi.org/10.1186/s42492-021-00092-y<\/li>\n<li>Weissgerber, T. L., Winham, S. J., Heinzen, E. P., Milin-Lazovic, J. S., Garcia-Valencia, O., Bukumiric, Z., Savic, M. D., Garovic, V. D., &amp; Milic, N. M. (2019). Reveal, don’t conceal: Transforming data visualization to improve transparency. Circulation, 140(18), 1506–1518. https://doi.org/10.1161/circulationaha.118.037777<\/li>\n<li>Wilke, C. O. (2019). Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures (1st ed.). O’Reilly Media.<\/li>\n<\/ul>\n","<ul>\n<li>Broad, W. J. (1981). The publishing game: getting more for less. Science, 211(4487), 1137–1139. https://doi.org/10.1126/science.7008199<\/li>\n<li>Hilgard, J., Sala, G., Boot, W. R., &amp; Simons, D. J. (2019). Overestimation of action-game training effects: Publication bias and salami slicing. Collabra. Psychology, 5(1), 30. https://doi.org/10.1525/collabra.231<\/li>\n<li>Kaiser, M., Drivdal, L., Hjellbrekke, J., Ingierd, H., &amp; Rekdal, O. B. (2021). Questionable Research Practices and misconduct among Norwegian researchers. Science and Engineering Ethics, 28(1), 2. https://doi.org/10.1007/s11948-021-00351-4<\/li>\n<li>Xie, J. S., &amp; Ali, M. J. (2023). To slice or perish. Seminars in Ophthalmology, 38(2), 105–107. https://doi.org/10.1080/08820538.2023.2172813<\/li>\n<\/ul>\n","<ul>\n<li>Fong, E. A., &amp; Wilhite, A. W. (2017). Authorship and citation manipulation in academic research. PloS One, 12(12), e0187394. https://doi.org/10.1371/journal.pone.0187394<\/li>\n<li>Holcombe, A. O. (2019). Contributorship, not authorship: Use CRediT to indicate who did what. Publications, 7(3), 48. https://doi.org/10.3390/publications7030048<\/li>\n<li>Wislar, J. S., Flanagin, A., Fontanarosa, P. B., &amp; Deangelis, C. D. (2011). Honorary and ghost authorship in high impact biomedical journals: a cross-sectional survey. BMJ , 343, d6128. https://doi.org/10.1136/bmj.d6128<\/li>\n<\/ul>\n","<ul>\n<li>Wicherts, J. M., Veldkamp, C. L., Augusteijn, H. E., Bakker, M., Van Aert, R., &amp; Van Assen, M. A. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. Frontiers in psychology, 1832. https://doi.org/10.3389/fpsyg.2016.0183<\/li>\n<\/ul>\n","<ul>\n<li>Boué, S., Byrne, M., Hayes, A. W., Hoeng, J., &amp; Peitsch, M. C. (2018). Embracing transparency through data sharing. International Journal of Toxicology, 37(6), 466–471. https://doi.org/10.1177/1091581818803880<\/li>\n<li>Ellis, S. E., &amp; Leek, J. T. (2018). How to share data for collaboration. The American Statistician, 72(1), 53–57. https://doi.org/10.1080/00031305.2017.1375987<\/li>\n<li>Quintana, D. S. (2020). A synthetic dataset primer for the biobehavioural sciences to promote reproducibility and hypothesis generation. eLife, 9. https://doi.org/10.7554/eLife.53275<\/li>\n<li>Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3, 160018. https://doi.org/10.1038/sdata.2016.18<\/li>\n<\/ul>\n","<ul>\n<li>Rosenthal, R. (1979). The file drawer problem and tolerance for null results. Psychological Bulletin, 86(3), 638–641. https://doi.org/10.1037/0033-2909.86.3.638<\/li>\n<li>Simonsohn, U., Nelson, L. D., &amp; Simmons, J. P. (2014). P-curve: A key to the file-drawer. Journal of Experimental Psychology: General, 143(2), 534–547. https://doi.org/10.1037/a0033242<\/li>\n<\/ul>\n"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>QRP<\/th>\n      <th>Alias(es) & related concepts<\/th>\n      <th>Definition<\/th>\n      <th>QRP umbrella term(s)<\/th>\n      <th>Research phase<\/th>\n      <th>Example(s)<\/th>\n      <th>Potential harms<\/th>\n      <th>Preventive measures<\/th>\n      <th>Detectability<\/th>\n      <th>Clues<\/th>\n      <th>Sources<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":40,"caption":"QRP Bestiary","searchHighlight":true,"autoWidth":true,"dom":"Bfrtip","buttons":["copy","excel","pdf","print"],"initComplete":"function(settings, json) {\n$('ul').css({'list-style-position': 'inside', 'margin': '0', 'padding': '0'});\n$('.dt-buttons').css({'margin-bottom': '10px'});\n$('td, th').css({'text-align': 'left', 'vertical-align': 'top'});\n}","columnDefs":[{"name":"QRP","targets":0},{"name":"Alias(es) & related concepts","targets":1},{"name":"Definition","targets":2},{"name":"QRP umbrella term(s)","targets":3},{"name":"Research phase","targets":4},{"name":"Example(s)","targets":5},{"name":"Potential harms","targets":6},{"name":"Preventive measures","targets":7},{"name":"Detectability","targets":8},{"name":"Clues","targets":9},{"name":"Sources","targets":10}],"order":[],"orderClasses":false,"orderCellsTop":true,"responsive":true,"lengthMenu":[10,25,40,50,100]}},"evals":["options.initComplete"],"jsHooks":[]}</script>
</body>
</html>
