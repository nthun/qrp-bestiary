
---
output: 
  word_document:
    reference_docx: ../docs/docx-template.docx
---
**Using measurement overlap to find significant results**  

_Definition_    

Overestimating the magnitude of an effect that is largely due to measurement overlap of two seemingly distinct constructs.  

_Alias(es) and related concepts_

Leveraging the jangle fallacy  

_Umbrella term(s)_

None  
  

_Examples_  

(1) Researcher is using similar items in two seemingly different constructs and concludes that the constructs have a high correlation.
(2) Researcher finds a positive relationship between depression and suicidality, but the positive relation is in part due to the depression scale already containing items about suicide.
(3) A researcher estimates a positive relation between construct A and construct B in a meta-analysis, but the meta-analysis includes studies that have used the same task to operationalize either construct.  

_Potential damages_  

- Inflated effect size estimates
- Inflated type I or type II error
- Reduced replicability    

_Remedies_  

- Be transparent about similar items
- Consider whether the measures used in a study distinctly operationalize different constructs
- Account for covariance due to similar items    

_Detectability_  

Yes

_Clues to detect_  

- Items or tasks are similar for the correlated constructs  


---

**Performing inappropriate power analysis**  

_Definition_    

Selection of inappropriate parameters or methods for power analysis and/ or misinterpreting/misusing power analysis results.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

None  
  

_Examples_  

(1) A researcher chooses a small sample size to get null results in a study about the harmful effects of smoking.
(2) A researcher uses default parameters for the power analysis to show that power analysis had been performed, however, the analysis is uninformative.  

_Potential damages_  

- Inflated confidence in the research
- Inflated type II error
- Reduced replicability    

_Remedies_  

- Be transparent about how power analysis was conducted
- Use meaningful parameters that are based on field standards, literature, or common sense    

_Detectability_  

Yes

_Clues to detect_  

- Details of power analysis not reported or justified
- Parameters of the power analysis are generic and do not fit the study
- Relatively low sample size
- The results of power analysis are misinterpreted  


---

**PARKing (preregistering after results are known)**  

_Definition_    

Preregistering a hypothesis or analysis after knowing the outcome of the analysis.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

Misusing open science practices  

_Examples_  

(1) Researcher realizes that their paper won't be accepted without a preregistration, so they create one post-hoc and link it to their study.  

_Potential damages_  

- Inflated confidence in the research    

_Remedies_  

- Be transparent about when the preregistration was done
- Disclose the familiarity (if any) of the researcher with the data
- Preregister before analyzing the data    

_Detectability_  

Yes

_Clues to detect_  

- Data collection occurred previous to the preregistration date
- Date of preregistration is unrealistically close to the first submission without the paper being a registered report  


---

**Using biased manipulations**  

_Definition_    

Using an unjustified manipulation to reach a misleading outcome.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

Influencing participants  

_Examples_  

(1) Selecting sub-clinical drug doses to suggest no effect or only deliberately high does to suggest an adverse effect. 
(2) Using images or videos to elicit emotions, but the stimuli are not eliciting emotions.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced generalizability    

_Remedies_  

- Run pilot studies to investigate if the manipulation can elicit an effect
- Use manipulation check questions
- Use standard stimuli or dosages    

_Detectability_  

Maybe

_Clues to detect_  

- Lack of manipulation check
- Using dosages outside of recommended values
- Using stimuli that can elicit extreme responses
- Using stimuli that were not previously tested and/or have no proven effect  


---

**Using biased measurements**  

_Definition_    

Assessing a construct with an instrument that is biased or invalid, in order to support the desired narrative.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

Influencing participants  

_Examples_  

(1) Researcher uses loaded, leading, or suggestive questions, e.g., "Does the lack of respect schoolchildren have for their teachers, in your opinion, influence everyday teaching methods in schools?"
(2) Researcher assesses a psychological construct using an ad hoc questionnaire with no proven validity.
(3) Researcher uses a scale that do not capture the intended construct properly in order to support a desired narrative  

_Potential damages_  

- Inflated or deflated effect size estimates 
- Inflated type I or type II error 
- Reduced replicability    

_Remedies_  

- Frame questions in a neutral way
- Get external opinions on questionnaires/study materials OR conduct a registered report
- Use questionnaires that are unbiased and psychometrically sound    

_Detectability_  

Yes

_Clues to detect_  

- Ad hoc questionnaires are used instead of validated instruments
Measurement items use suggestive or biased language
- Measurement is based on single-item questions
- No discussion of the psychometric properties of the instruments  


---

**Selective sampling**  

_Definition_    

Choosing a sample in a way that biases the findings.  

_Alias(es) and related concepts_

Biased sampling  

_Umbrella term(s)_

Sample curation  

_Examples_  

(1) Researcher tests the likeability of chocolate on a group of children only in order to find that everyone loves it.
(2) Using uncomparable groups: The researcher tests if men are more aggressive than women. For comparison, women from a university are compared with men from a prison.
(3) Picking a subsample of a panel dataset to find the desired results.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced generalizability
- Reduced replicability if sampling bias is not disclosed    

_Remedies_  

- Consider using statistical control for confounding variables
- Make sure that the sample represents the population
- Preregister the sampling process
- Report the sampling process transparently
- Use a sampling method that doesn't bias the results
- Use comparable (or matched) groups    

_Detectability_  

Yes

_Clues to detect_  

- Compared groups are from different populations
- Convenience sample
- Unclear rationale for sample selection  


---

**Optional stopping**  

_Definition_    

Monitoring hypothesis tests during data collection, and stopping when statistical inference is favorable, without controlling for sequential testing.  

_Alias(es) and related concepts_

Peeking, 
Data peeking  

_Umbrella term(s)_

Sample curation  

_Examples_  

(1) Researcher is collecting responses and tests the hypothesis after every participant - when significance is reached, the researcher stops collecting data.  

_Potential damages_  

- Inflated type I error
- Reduced replicability    

_Remedies_  

- Preregister stopping rules and adjustments for type I error-inflation 
- Preregister the estimated sample size    

_Detectability_  

Maybe

_Clues to detect_  

- Absence of preregistration
- Low sample size
- P-values are just below the significance threshold (usually 0.05)
- Relatively large effect size compared to other studies in the field
- Vague or absent reason for sample size  


---

**Retaining pilot data**  

_Definition_    

Including data from a pilot study if the results support the hypothesis.  

_Alias(es) and related concepts_

Double dipping  

_Umbrella term(s)_

Sample curation  

_Examples_  

(1) Researcher conducts a pilot study to check the protocol and analyzes pilot data. The pilot data and main study data are aggregated in the data analysis if pilot data results are in line with expectations.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced replicability    

_Remedies_  

- Don't include pilot data in the analyzed dataset
- Report pilot data separately    

_Detectability_  

Maybe

_Clues to detect_  

- If data are shared, timestamps may fall into two distinct periods
- Sample sizes reported throughout the paper might not match  


---

**Grooming participants**  

_Definition_    

Affecting participants to make them give responses that support the desired narrative.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

Influencing participants  

_Examples_  

(1)     Researcher tells the participants that he believes the treatment will work.
(2)     Researcher uses a briefing document that is trying to influence participants’ attitudes about the topic that is assessed in the study.
(3)     During data collection the same organization that runs the study also runs a marketing campaign to influence public opinion on the same topic.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced replicability    

_Remedies_  

- Avoid exposing participants to any cues that might influence their responses
- Blind the experimenter where possible
- Researchers interacting with participants should remain neutral and follow scripts during testing
- Use automated research procedures (e.g. research presentation software) instead of human research assistants wherever possible    

_Detectability_  

No

_Clues to detect_  

- Absence of blinding
- Absence of procedure scripts
- Suggestive questions in the survey  


---

**Missing data hacking**  

_Definition_    

Choosing the strategy to handle missing data based on the impact on the results.  

_Alias(es) and related concepts_

Favorable imputation  

_Umbrella term(s)_

P-hacking  

_Examples_  

(1)     A researcher tries three ways of handling missing data, for example, listwise deletion, multiple imputation, and inverse probability weighting. The expected results only appear with inverse probability weighting. The researcher reports only this strategy in the paper and leaves out results with listwise deletion and multiple imputation.
(2)     Can also be within a single method, specifically multiple imputation, since it uses one or more variables to replace missing data, and the choice of these variables is up to the researcher, but can also be statistically based.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced reproducibility
- Reduced replicability    

_Remedies_  

- Perform blinded data analysis
- Perform sensitivity analysis
- Preregister missing data approach    

_Detectability_  

Maybe

_Clues to detect_  

- Absence of open data
- Lack of rationale or references supporting the missing data approach
- No mention of the missing data approach or the missing data at all
- Unexplained discrepancy between the recruited and analyzed sample sizes  


---

**Using ad hoc exclusion criteria for participants**  

_Definition_    

Exclusion of participants without proper justification and transparent reporting.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

P-hacking
Sample curation  

_Examples_  

(1) Researcher finds that a correlation between two variables is not significant. After removing two participants - who should be included - the association becomes significant. Then the researcher comes up with post hoc exclusion criteria for those participants.
(2) A researcher doesn’t find an expected association between perceived stress and personality. When looking only at the top 25% of perceived stress scores, the association is there. They go on to report the top 25% scores as their population of interest and do not disclose that they looked at the rest of the sample population.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced generalizability
- Reduced replicability
- Reduced reproducibility    

_Remedies_  

- Perform blinded data analysis 
- Perform sensitivity analysis
- Preregister the study
- Report post hoc changes in exclusion criteria    

_Detectability_  

Maybe

_Clues to detect_  

- Absence of open data
- Absence of preregistration
- Sample too narrow for recruitment methods
- Unexplained discrepancy between the recruited and analyzed sample sizes  


---

**Excluding data points**  

_Definition_    

Exclusion of data points or outliers without proper justification and transparent reporting.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

P-hacking  

_Examples_  

(1) Removing individual reaction time trials based on post hoc criteria.
(2) Trying different outlier cut-off criteria until an effect is statistically significant.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced generalizability
- Reduced replicability
- Reduced reproducibility    

_Remedies_  

- Perform blinded data analysis
- Perform sensitivity analysis 
- Preregister the study
- Report post hoc changes in exclusion criteria    

_Detectability_  

Maybe

_Clues to detect_  

- Absence of open data
- Absence of preregistration
- Unexplained discrepancy between the recruited and analyzed sample sizes and degrees of freedom  


---

**Redefining group membership rules**  

_Definition_    

Post hoc (re)definition of grouping criteria without proper justification and transparent reporting.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

P-hacking  

_Examples_  

(1) Collapsing the multicategorical variable of sexual orientation into heterosexual and non-heterosexual.
(2) Trying different age ranges in cross-sectional age comparisons to maximize group differences.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced replicability
- Reduced reproducibility
- Reduced generalizability    

_Remedies_  

- Report post hoc changes in grouping rules and report results using original grouping rules as well
- Preregister the study
- Perform blinded data analysis
- Perform sensitivity analysis    

_Detectability_  

Maybe

_Clues to detect_  

- Absence of open data
- Absence of preregistration
- Oversimplified sample description
- Response options in materials/methods different than reported groups  


---

**Discretizing continuous variables**  

_Definition_    

Taking a continuous variable and making it categorical without proper justification and transparent reporting.  

_Alias(es) and related concepts_

Dichotimizing variables, 
Median split  

_Umbrella term(s)_

P-hacking  

_Examples_  

(1)     Researcher doesn’t find an association between depression and continuous age variables, and recodes age into young and old categories. After that, age groups show a significant association with depression. An independent samples t-test is reported instead of a correlation.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced replicability
- Reduced generalizability    

_Remedies_  

- Perform blinded data analysis
- Perform sensitivity analysis
- Preregister the study
- Use original measurement levels    

_Detectability_  

Yes

_Clues to detect_  

- Absence of open data
- Scale or response options in materials or methods do not match how they are reported in the results
- Test statistics do not match expected data analysis strategy  


---

**Modifying measurements**  

_Definition_    

Changing the properties of a measure/measurement to produce favorable results without proper justification and transparent reporting.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

P-hacking  

_Examples_  

(1) Researcher uses only a portion of the items from a longer scale.
(2) Researcher combines items from different scales into a single measure.
(3) Researcher chooses which EEG electrodes to aggregate based on the results.  

_Potential damages_  

- Reduced replicability
- Reduced reproducibility
- Reduced validity of the measure
- Inflated or deflated reliability of the measure
- Inflated type I or type II error
- Inflated or deflated effect size estimates    

_Remedies_  

- Describe and justify any modifications on measurements
- Perform blinded data analysis
- Perform sensitivity analysis
- Publish study materials
- Preregister the study
- Use conventional measurements/measures    

_Detectability_  

Maybe

_Clues to detect_  

- Absence of open data
- Absence of open study materials
- Discrepancy between the reported version of measurement/measure and original or conventional measure  


---

**Variable transformation fishing**  

_Definition_    

Selecting variable transformations that produce favorable results without proper justification and transparent reporting.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

P-hacking  

_Examples_  

(1) Researcher runs a statistical test using several different transformations (e.g., changing levels of measurement, log-transformations, rescaling) of the outcome, and only reports the one that produces a significant result.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced replicability
- Reduced reproducibility    

_Remedies_  

- Describe and justify any variable transformations
- Perform blinded data analysis
- Perform sensitivity analysis
- Preregister conditional transformations    

_Detectability_  

Maybe

_Clues to detect_  

- Abscence of open data
- Reported values are outside of regular range
- Transformation is applied without justification
- Using transformations that are unconventional for the measure  


---

**Using ad hoc covariates**  

_Definition_    

Addition or removal of covariates to influence the estimates or significance for the effect of interest.  

_Alias(es) and related concepts_

Selectively including control variables  

_Umbrella term(s)_

P-hacking  

_Examples_  

(1) A researcher opportunistically decides which background variables (e.g., age, gender) to control for, without a causal theory or a preregistration.
(2) A researcher decides whether to control for a baseline value in an experimental design depending on the results of statistical tests.
(3) Researcher avoids the inclusion (or measurement) of theoretically justified moderators (e.g., severity of a condition, or socio-economic status) to be able to imply greater generalisability.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced generalizability
- Reduced replicability
- Reduced reproducibility    

_Remedies_  

- Perform blinded data analysis
- Preregister complete models/analytical plan
- Report robustness checks    

_Detectability_  

Maybe

_Clues to detect_  

- Different covariates in different analysis steps are used
- There is a lack of justification for the selection of the covariates  


---

**Choosing unjustified p-value adjustment**  

_Definition_    

Not adjusting or over-adjusting p-values when running multiple tests.  

_Alias(es) and related concepts_

Not adjusting or over-adjusting p-values  

_Umbrella term(s)_

P-hacking  

_Examples_  

A researcher decides (1) whether or not to adjust for multiple tests (e.g., in an ANOVA) and (2) which adjustment method to use, and (3) which (i.e. how many) comparisons to include depending on results obtained. 
(4) Researcher uses Bonferroni correction when correlating several variables, to prove that an association does not exist.  

_Potential damages_  

- Inflated type I or type II error    

_Remedies_  

- Perform blinded data analysis
- Perform sensitivity analysis 
- Preregister p-value adjustment plans
- Preregister planned contrasts    

_Detectability_  

Yes

_Clues to detect_  

- Multiple tests are made that would require p-value adjustment
- P-value adjustment not mentioned
- Using p-value correction that is too strict (e.g., Bonferroni) without proper justification  


---

**Choosing a poor model specification**  

_Definition_    

Creating too complex models on too small datasets causing the model to learn the noise and random fluctuations instead of generalizable patterns. Alternatively, creating too simple models that do not adequately fit the data.  

_Alias(es) and related concepts_

Overfitting or underfitting models,
Bias-variance tradeoff  

_Umbrella term(s)_

P-hacking  

_Examples_  

(1)     Overfitting: The researcher fits a regression model with 25 predictors on a sample of 100 participants.
(2)     Underfitting: The researcher uses linear regression to investigate a non-linear association.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error    

_Remedies_  

- Perform blinded data analysis
- Perform sensitivity analysis 
- Use a theoretically justified model in confirmatory studies
- Underfitting: Visualize data and the model
- Use methods that prevent overfitting in exploratory research, e.g. use separate train and test datasets, use cross-validation resampling methods, use regularization or other feature selection methods    

_Detectability_  

Yes

_Clues to detect_  

- Improper prediction selection (e.g., no regularization)
- No mention of holdout (or test) dataset or cross-validation
- Overfitting: Very high R2 value (close to 1)
- The number of included predictors in the model is large
- The number of observations is low
- Underfitting: Data visualization shows high model bias  


---

**Neglecting assumptions for statistical models**  

_Definition_    

Using statistical models although requirements are not met.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

P-hacking  

_Examples_  

(1) Analyzing data, using parametric tests such as t-tests but the data requires a non-parametric test.
(2) Analyzing dependent data using a statistical model that does not account for dependency.  

_Potential damages_  

- Inflated type I or type II error
- Reduced replicability
- Reduced reproducibility    

_Remedies_  

- Perform and report necessary assumption checks    

_Detectability_  

Yes

_Clues to detect_  

- Evidence of assumption breaches (e.g., non-normality, non-independent data, largely different SDs by group)
- Not reporting assumption checks  


---

**Selecting a favorable random number generator seed**  

_Definition_    

Trying different random seeds until getting a favorable result, potentially in combination with small number of replications  

_Alias(es) and related concepts_

Resampling lottery  

_Umbrella term(s)_

P-hacking  

_Examples_  

(1) A researcher keeps on bootstrapping a confidence interval (e.g., for a mediation indirect effect) with different seeds until the 95% confidence interval just excludes 0.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced replicability
- Reduced reproducibility    

_Remedies_  

- Only report significance if the results are robust across random seeds
- Use a large number of replications (e.g., bootstrap samples)
- Perform blinded data analysis    

_Detectability_  

Maybe

_Clues to detect_  

- P-values just below the significance threshold (usually 0.05)  


---

**Selective test reporting**  

_Definition_    

Repeatedly testing a hypothesis in different ways until the desired result is found, and then selectively reporting the findings that support the desired conclusion.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

P-hacking
Cherry-picking  

_Examples_  

Researcher analyzes the data using (1) multiple statistical methods (multiple t-tests, ANOVAs, different random structures in LMEMs) and/ or (2) multiple data eligibility specifications. Based on the results, they choose to present only one analysis that gives a significant result.  

_Potential damages_  

- Inflated confidence in the research
- Inflated type I or type II error
- Reduced reproducibility
- Reduced replicability    

_Remedies_  

- Perform blinded data analysis
- Perform specificity curve analysis
- Preregister data processing (e.g., missing data approach) and statistical analysis strategy
- Report all performed hypothesis-tests    

_Detectability_  

Maybe

_Clues to detect_  

- Absence of preregistration
- Arbitrary data processing steps and/or statistical methods
- P-values just below the significance threshold (usually 0.05)  


---

**Making unsupported conclusions**  

_Definition_    

Interpreting research findings or their implications in a way that is not backed by evidence.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

None  
  

_Examples_  

(1) Researcher concludes that a treatment is effective for groups and contexts that were not considered in the study. 
(2) Researcher concludes that a treatment worked, however, the treatment effect did not differ from the effect of the control condition (or no control condition was used). 
(3) Researcher implies causality based on a research design or that does not allow causal inference.  

_Potential damages_  

NA    

_Remedies_  

- Make it clear that the evidence is limited to certain contexts
- Make sure that every interpretation is properly supported by evidence 
- Use conditional statements where evidence is weak or the researcher uses extrapolation    

_Detectability_  

Yes

_Clues to detect_  

- Causal claims are made without the methodology or analysis allowing causal inference
- Results are generalized to contexts outside of the study's scope
- The chosen methodology and statistical analysis do not allow to answer the hypothesis
- The statistical results do not match the conclusions  


---

**HARKing (hypothesizing after the results are known)**  

_Definition_    

Presenting a hypothesis that is based on observed results (post-hoc or a posteriori) as if it was presumed before obtaining results (a priori).  

_Alias(es) and related concepts_

Texas sharpshooter fallacy, 
Post hoc ergo propter hoc  

_Umbrella term(s)_

None  
  

_Examples_  

(1) Researcher claims to have predicted an unexpected result. 
(2) Researcher has no hypotheses originally and forms hypotheses after exploring the data and presenting the hypotheses as they had those from the beginning.
(3) Researcher has a hypothesis (e.g., a mediation hypothesis) and tests it, and if the results do not confirm the hypothesis but rather indicate an alternative pattern (e.g., a moderation), the researcher claims that this is what they hypothesized all along.
(4) Post-hoc directional hypotheses: The researcher presents a hypothesis as if it was uni-directional (i.e. group A’s mean is larger than group B’s, or a correlation will be positive), although the original hypothesis was bi-directional. This change will make the hypothesis test significant.  

_Potential damages_  

- Inflated confidence in the research
- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced replicability    

_Remedies_  

- Clearly separate exploratory and confirmatory findings
- Form hypotheses before analyzing the data
- Perform blinded data analysis
- Preregister confirmatory hypotheses
- Use robust exploratory research practices (e.g. holdout dataset, cross-validation, multiverse analysis, blinded data analysis, etc.)    

_Detectability_  

Maybe

_Clues to detect_  

- Absence of preregistration
- Unexplained and unconventional choices in the methods and results section  


---

**Incorrect reporting of test statistics**  

_Definition_    

Not using statistical test reporting conventions to obscure exact results and assume that they are above or below threshold values.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

None  
  

_Examples_  

(1)     Researcher is ‘rounding off’ a p-value in a paper (e.g., reporting that a p-value of .054 is less or equal to .05).
(2)     Researcher reports p-values only and conceals test statistics.
(3)     Researcher reports a correlation without disclosing the degrees of freedom, number of observations, or confidence interval, so it seems like the effect is large (for example r=.70, n=15, CI=[.01; .90]).
(4)     Researcher does a model comparison and only reports fit statistics that are in favor of the preferred model.  

_Potential damages_  

- Inflated type I or type II error
- Reduced reproducibility    

_Remedies_  

- Adhere to reporting conventions (e.g., APA)
- Publish data
- Publish processing and analysis code
- Use literate programming (e.g., RMarkdown, quarto, jupyter)
- Work with software that supports you in producing and checking your write-up (e.g., papaja, stat-check)    

_Detectability_  

Yes

_Clues to detect_  

- Absence of open code
- Absence of open data
- Anomalies in reported statistics, e.g., test statistics are incompatible with p-values
- Fit statistics are missing without proper explanation
- Statistics are not reported according to conventions (e.g., three digits for p-values, reporting of df)  


---

**Visualizing data in a misleading way**  

_Definition_    

Choosing suboptimal visualizations or altering figure properties in order to exaggerate or diminish effects.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

None  
  

_Examples_  

(1) Researcher truncates the y-axis so it is not starting at zero and/or does not add error bars. This makes differences seem larger and more significant than they are in reality.
(2) Researcher uses arbitrary categories to present interval data on a map.
(3) Researcher displays a pie chart with percentage numbers falling below or exceeding 100.  

_Potential damages_  

NA    

_Remedies_  

- Follow best practices on how to visualize data    

_Detectability_  

Yes

_Clues to detect_  

- Chartjunk (e.g., 3D elements, ornaments) is present on the plot
- In a plot y-axis is starting at an arbitrary point
- Only summary statistics are shown without individual data points
- Scale or response options in text do not match how they are presented in a plot
- Statistical uncertainty (e.g., error bars) is not shown on plots
- Visualization does not match the reported results in the text  


---

**Selective reporting of hypotheses**  

_Definition_    

Reporting hypothesis test only if it fits the researcher's expectation.  

_Alias(es) and related concepts_

Cherry-picking hypotheses, 
Chrysalis effect, 
Fishing expedition  

_Umbrella term(s)_

Cherry-picking  

_Examples_  

(1) Researcher formulates five hypotheses of which only three are supported by the data - only these 3 get reported in the final research report (Chrysalis effect).
(2) Fishing expedition - The researcher surveys college students about the outfit they are wearing and their scores on several tests which allows for many possible analyses (examining different colors, types of clothing, tests, score cutoffs, etc.). They end up reporting only a subset of findings to claim college students perform significantly better on tests when they are wearing green. See also Modifying measurement, Selective reporting of indicator variables, and Selective reporting of outcomes.  

_Potential damages_  

- Inflated confidence in the research
- Inflated or deflated effect size estimates
- Inflated type I or type II error    

_Remedies_  

- If some hypotheses get left out due to the scope of the write-up be transparent about it
- Preregister the study
- Report all hypotheses in the write-up regardless of whether they were confirmed or not    

_Detectability_  

Maybe

_Clues to detect_  

- Number of hypotheses in preregistration (or dissertation) exceeds the number in the publication  


---

**Omitting important details of the scientific process**  

_Definition_    

Not reporting important details of the methodology and statistical analysis.  

_Alias(es) and related concepts_

Incomplete methods or results section  

_Umbrella term(s)_

Cherry-picking  

_Examples_  

(1) Researcher omits sample characteristics.
(2) Researcher reports correlations without specifying the type, e.g., Spearman.
(3) Researcher reports having conducted an online study and does not reveal it was an MTurk-study.
(4) Researcher does not fully disclose compensation etc. for participants during data collection. 
(5) Researcher does not share study materials on request.
(6) Researcher does not report exact questionnaire items.  

_Potential damages_  

- Reduced generalizability
- Reduced replicability
- Reduced reproducibility    

_Remedies_  

- Preregister the study or written research plan before conducting the study
- Report every important detail of the scientific process
- Use a lab log during data collection to keep track of changes in the scientific process    

_Detectability_  

Maybe

_Clues to detect_  

- Absence of open study materials
- Details that are usually shared are missing
- Replication is not possible from published methods  


---

**Selective reporting of indicator variables**  

_Definition_    

Reporting only the indicator variables (or predictors, features, independent variables) that are used in analyses that produce expected results.  

_Alias(es) and related concepts_

Cherry-picking indicator variables, 
Cherry-picking conditions/ groups  

_Umbrella term(s)_

Cherry-picking  

_Examples_  

(1) Researcher reports IVs that are associated with the outcome rather than including all measured IVs in the results section.
(2) Researcher drops one or more conditions or groups/merges two or more groups into one / splits a group into more groups than were initially planned depending on statistical results.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced generalizability
- Reduced replicability    

_Remedies_  

- Perform blinded data analysis
- Preregister the study
- Report all indicators    

_Detectability_  

Maybe

_Clues to detect_  

- Indicators reported in Supplemental Material but not mentioned in main text
- Measures get reported in the methods section but not in the results section
- Number of preregistered indicators exceeds the number of indicators in publication
- Reported mean time of participation does not match the number of reported measures  


---

**Selective reporting of outcomes**  

_Definition_    

Reporting only the outcomes (or dependent variables) that are used in analyses that produce expected results.  

_Alias(es) and related concepts_

Cherry-picking outcomes  

_Umbrella term(s)_

Cherry-picking  

_Examples_  

(1) Researcher uses several scales to measure the same construct but only reports the one that produces expected results.
(2) Researcher tests effectiveness of a new intervention for depression by measuring its effects on anxiety, sleep quality, and stress and only reports the outcome that shows the desired effect.  

_Potential damages_  

- Inflated or deflated effect size estimates
- Inflated type I or type II error
- Reduced generalizability
- Reduced replicability    

_Remedies_  

- Perform blinded data analysis
- Preregister the study
- Report all outcomes    

_Detectability_  

Maybe

_Clues to detect_  

- Measures get reported in the methods section but not in the results section 
- Number of preregistered outcomes exceeds number outcomes in publication
- Outcomes reported in Supplemental Material but not mentioned in main text
- Reported mean time of participation does not match number of reported measures  


---

**Using unjustified references**  

_Definition_    

Selectively citing works by specific researchers or journals to inflate citation metrics or boost a journal’s impact factor.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

Citation engineering  

_Examples_  

(1) Researcher selectively cites their own publications for boosted citation metrics. 
(2) Citation networks: Researcher cites a colleague’s unrelated work in order to get cited in a similar way.  

_Potential damages_  

- Inflated credibility of publications
- Inflated credibility of journals    

_Remedies_  

- Cite only relevant studies
- Provide comprehensive coverage of related scholarly literature    

_Detectability_  

Yes

_Clues to detect_  

- Publications are cited without relevance to the claims
- Specific authors or journals are cited disproportionately frequently  


---

**Citing unreliable research**  

_Definition_    

Citing an unreliable publication to support the study's narrative.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

Citation engineering  

_Examples_  

(1) Researcher cites a publication that presents low-level evidence to support a claim with no reference to the study’s limitations or no reference to other studies. 
(2) The researcher cites a retracted paper.  

_Potential damages_  

- Increasing the credibility of low-evidence research
- Inflated credibility of statements    

_Remedies_  

- Never cite a retracted study
- Only cite publications that properly support their claims    

_Detectability_  

Yes

_Clues to detect_  

- The cited publication provides no or low-quality evidence to its claims
- The cited publication is retracted or otherwise discredited or its claims are refuted  


---

**Using irrelevant references**  

_Definition_    

Using citations that are not connected to the claims to increase the credibility of a statement.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

Citation engineering  

_Examples_  

(1) Researcher supports a statement with three citations and two of them are unrelated to the statement.  

_Potential damages_  

- Inflated credibility of statements    

_Remedies_  

- Cite only relevant studies    

_Detectability_  

Yes

_Clues to detect_  

- Publications are cited without relevance to the claims  


---

**Selective citing**  

_Definition_    

Avoiding to mention studies that do not support the hypothesis of the research or even those that do support the hypotheses to make the study appear more novel.  

_Alias(es) and related concepts_

Cherry-picking citations  

_Umbrella term(s)_

Citation engineering
Cherry-picking  

_Examples_  

(1) Researcher overly cites empirical work that supports their hypotheses and withholds citing work that did not find the effect at all or even the opposite. 
(2) Researcher omits other null findings to maximize the perceived value of a null finding.  

_Potential damages_  

- Inflated credibility of statements
- Inflated confidence in the research    

_Remedies_  

- Provide comprehensive coverage of related scholarly literature    

_Detectability_  

Yes

_Clues to detect_  

- Cited studies only point into one direction 
- Important studies and experts are missing from references
- Systematic reviews and meta-analyses are not cited  


---

**Not disclosing deviations from preregistration**  

_Definition_    

Deviating from the preregistration without transparency and proper justification in the publication.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

Misusing open science practices  

_Examples_  

(1) Researcher preregisters that they would collect data from a non-student sample, but ends up including students, and does not disclose this deviation.
(2) Researcher preregisters a data analysis using linear regression but used robust regression instead without reporting the discrepancy.
(3) See also Example 1 in Selective reporting of hypotheses.  

_Potential damages_  

- Inflated confidence in the research
- Reduced replicability    

_Remedies_  

- Avoid vagueness in preregistration
- Disclose and justify every divergence from the preregistration
- Use methods that will provide robust results even when preregistration is not specific at points (e.g. blind data analysis, cross-validation)    

_Detectability_  

Yes

_Clues to detect_  

- Link to the preregistration in the manuscript does not work or leads to a page that cannot be accessed
- Preregistration and published study differ on important aspects  


---

**Not linking the preregistration to the published study**  

_Definition_    

Creating a preregistration but not associating it with the published study.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

None  
  

_Examples_  

(1) Researcher preregisters a study and after conducting the research the preregistration is not mentioned in the manuscript because of too many diversions.  

_Potential damages_  

NA    

_Remedies_  

- Always link the preregistration to the manuscript and report discrepancies    

_Detectability_  

Maybe

_Clues to detect_  

- A preregistration that fits the study is findable  


---

**Not making data accessible**  

_Definition_    

The datasets and/or codebooks are not made accessible to the public and/or peer-reviewers without justifiable cause.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

Misusing open science practices  

_Examples_  

(1) Researcher doesn’t provide a publicly accessible repository link to the dataset. 
(2) Data repository link is accessible, but the data is not comprehensible (e.g. lacks cleaning and organization, codebook and instructions, etc), hence it is difficult or impossible to use to reproduce findings.  

_Potential damages_  

- Prevents data reuse
- Reduced reproducibility    

_Remedies_  

- Data should be shared based on the FAIR (findable, accessible, interoperable, and reusable) principles and legislative context of the researcher 
- If confidential and personal information makes participants identifiable, apply masking and anonymization, and then share data
- Use synthetic datasets when original data can't be shared    

_Detectability_  

Yes

_Clues to detect_  

- Data are not shared according to FAIR principles
- No information in the publication on the availability of the data  


---

**Publishing studies selectively**  

_Definition_    

Choosing which study to publish or share based on whether the findings fit expectations.  

_Alias(es) and related concepts_

File drawer problem  

_Umbrella term(s)_

Cherry-picking  

_Examples_  

(1) Researcher runs a study and finds out the results do not support their hypothesis (e.g., no significant findings). Thus the researcher does not try to publish or share the study publicly.
(2) Researcher runs several studies, and publishes only those that support the hypothesis in a multi-study paper.  

_Potential damages_  

- Creates inflated confidence in a multi-study paper 
- Creates publication bias    

_Remedies_  

- Only preregister on platforms, that will eventually publish all preregistrations
- Publish all studies, even when the findings do not support hypotheses    

_Detectability_  

No

_Clues to detect_  

- Publication bias can be estimated in meta-analysis  


---

**Declaring false authorship**  

_Definition_    

Attribution and arrangement of authorship that does not correspond to the authors’ contributions, in order to influence the publishing process, and increase the credibility of the study.  

_Alias(es) and related concepts_

-  
  

_Umbrella term(s)_

Citation engineering  

_Examples_  

(1) Honorary authorship: Researcher adds a co-author who did not contribute to the manuscript.
(2) Ghost authorship: The researcher excludes a co-author who significantly contributed to the project.
(3) Controversial researcher writes a paper and publishes it under a pseudonym, so it seems that more than one person shares the same view.  

_Potential damages_  

- Contributing authors may not get the credit they ought to get
- Inflated confidence in the research based on the reputation of authors who were included in (or excluded from) the author list    

_Remedies_  

- Explicitly declare contributions to the project (e.g., CRediT taxonomy)
- Include everyone who made a significant contribution to the project
- Only include authors who contributed to the project    

_Detectability_  

No

_Clues to detect_  

None  
  


---

**Creating multiple publications from the same study**  

_Definition_    

Breaking up of research findings from the same dataset into several publications without proper justification and the disclosing of related papers.  

_Alias(es) and related concepts_

Salami slicing  

_Umbrella term(s)_

Citation engineering  

_Examples_  

(1) Researcher conducts a study measuring several outcomes (or predictors) and publishes results in several papers with each paper focusing on just one outcome (or predictor), while not disclosing the other papers. 
(2) A study on cross-cultural differences with 20 participating labs from 20 countries results in 10 publications where in each one two countries are compared.  

_Potential damages_  

- Biased effect size estimates in meta-analyses (due to non-independence of results)
- Inflated type I or type II error due to unknown family-wise error rate    

_Remedies_  

- Preregister the publication strategy
- Publish study results in one single publication or disclose all related papers    

_Detectability_  

Maybe

_Clues to detect_  

- Absence of open data
- Description of the sample is the same over several studies by the same researcher or lab
- Several papers exist with similar outcomes or predictors based on the same dataset by the same researcher or lab
- The methods suggest a large study but the scope of the paper is narrow  


---

